{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f60fa1-7eb7-4e33-8b2d-d4d6025b0da6",
   "metadata": {
    "tags": []
   },
   "source": [
    "*KTH Royal Institute of Technology* \\\n",
    "DD2424 Deep Learning in Data Science | Project (grade E part)\\\n",
    "Diogo Paulo 030224-8216 (diogop@kth.se)\\\n",
    "Hugo Dezerto 20011224-8257 (hugoad@kth.se) \\\n",
    "Maria Sebasti√£o 031010-T207 (mcms2@kth.se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f336f023-9bfb-4ea6-84da-1c2acb7afab4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grade E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b3315ba-0c7b-44c0-9418-d1d3bec4fab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/grade_E'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to check if the current working directory is correct\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27642bdc-7bfc-4c86-a0d1-109542e774f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "from torch.utils.data import random_split, Dataset\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set device. Use GPU if available else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20fd2f-8437-4766-909a-f73a96a14eb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ecada7d-a951-4319-84c9-87dddc6d1c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Number of training samples: 3680 Number of test samples: 3669\n",
      "ResNet18 model loaded.\n",
      "Epoch 1, Loss: 0.1506\n",
      "Epoch 2, Loss: 0.0655\n",
      "Epoch 3, Loss: 0.0554\n",
      "Epoch 4, Loss: 0.0579\n",
      "Epoch 5, Loss: 0.0461\n",
      "Epoch 6, Loss: 0.0479\n",
      "Epoch 7, Loss: 0.0416\n",
      "Epoch 8, Loss: 0.0423\n",
      "Epoch 9, Loss: 0.0393\n",
      "Epoch 10, Loss: 0.0345\n",
      "Epoch 11, Loss: 0.0442\n",
      "Epoch 12, Loss: 0.0354\n",
      "Epoch 13, Loss: 0.0279\n",
      "Epoch 14, Loss: 0.0283\n",
      "Epoch 15, Loss: 0.0400\n",
      "Epoch 16, Loss: 0.0367\n",
      "Epoch 17, Loss: 0.0330\n",
      "Epoch 18, Loss: 0.0366\n",
      "Epoch 19, Loss: 0.0315\n",
      "Epoch 20, Loss: 0.0311\n",
      "Epoch 21, Loss: 0.0337\n",
      "Epoch 22, Loss: 0.0269\n",
      "Epoch 23, Loss: 0.0294\n",
      "Epoch 24, Loss: 0.0236\n",
      "Epoch 25, Loss: 0.0328\n",
      "Epoch 26, Loss: 0.0243\n",
      "Epoch 27, Loss: 0.0220\n",
      "Epoch 28, Loss: 0.0258\n",
      "Epoch 29, Loss: 0.0194\n",
      "Epoch 30, Loss: 0.0500\n",
      "Training time: 11.91 minutes\n",
      "Test Accuracy: 99.0188%\n"
     ]
    }
   ],
   "source": [
    "# --------------------- BINARY CLASSIFICATION ---------------------\n",
    "\n",
    "# Transform: Resize, normalize (ImageNet mean/std)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),        # Resize shortest side to 224, keep aspect ratio\n",
    "    transforms.CenterCrop(224),    # Crop from the center to 224x224\n",
    "    transforms.ToTensor(), # Convert to tensor\n",
    "    # Normalize with ImageNet mean and std (check https://pytorch.org/hub/pytorch_vision_resnet/)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset as binary classification problem (cat vs dog). The resulting object is a list of tuples (image, label)\n",
    "train_dataset = OxfordIIITPet(root='./dataset', split='trainval', target_types='binary-category', transform=transform, download=True)\n",
    "test_dataset = OxfordIIITPet(root='./dataset', split='test', target_types='binary-category', transform=transform, download=True)\n",
    "print(\"Dataset loaded. Number of training samples:\", len(train_dataset), \"Number of test samples:\", len(test_dataset))\n",
    "\n",
    "# Create DataLoader objects to efficiently load data in batches.\n",
    "# - train_loader: loads training data in batches of 32 and shuffles the data each epoch (improves generalization).\n",
    "# - test_loader: loads test data in batches of 32 without shuffling (for consistent evaluation).\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Load a ResNet18 model pre-trained on ImageNet\n",
    "model = resnet18(weights='IMAGENET1K_V1')\n",
    "print(\"ResNet18 model loaded.\")\n",
    "\n",
    "# Freeze all the parameters in the pre-trained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer to output 2 classes (cat vs dog)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "# Move the model to the selected device (GPU if available, else CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.002) # TUNE\n",
    "\n",
    "# Training loop\n",
    "def train_model(num_epochs):\n",
    "    model.train()  # Set model to training mode (enables dropout, batchnorm updates)\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0  # Accumulate loss for this epoch\n",
    "        for imgs, labels in train_loader:  # Loop over each batch in the training data\n",
    "            imgs, labels = imgs.to(device), labels.to(device)  # Move data to GPU or CPU\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            outputs = model(imgs)  # Forward pass: compute model predictions\n",
    "            loss = criterion(outputs, labels)  # Compute loss between predictions and true labels\n",
    "            loss.backward()  # Backward pass: compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            running_loss += loss.item()  # Add batch loss to epoch total\n",
    "        # Print average loss for this epoch\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "def test_model():\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, uses running stats for batchnorm)\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency during evaluation\n",
    "        for imgs, labels in test_loader:  # Iterate over the test dataset in batches\n",
    "            imgs, labels = imgs.to(device), labels.to(device)  # Move data to the appropriate device (CPU or GPU)\n",
    "            outputs = model(imgs)  # Get model predictions (logits) for the batch. shape: (batch_size, 2)\n",
    "            _, preds = torch.max(outputs, 1)  # Get the predicted class (index of max logit) for each sample\n",
    "            correct += (preds == labels).sum().item()  # Count how many predictions are correct in this batch\n",
    "            total += labels.size(0)  # Update the total number of samples seen so far\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.4f}%\")\n",
    "\n",
    "\n",
    "# Run training and testing\n",
    "start_time = time.time() # <--- RECORD START TIME\n",
    "train_model(num_epochs=30) # TUNE\n",
    "end_time = time.time() # <--- RECORD END TIME\n",
    "duration = end_time - start_time\n",
    "print(f\"Training time: {duration/60:.2f} minutes\")\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e024d0f-8475-4d1e-bb33-05e8e3eab9e2",
   "metadata": {},
   "source": [
    "**Test Accuracy**: 99.0188%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da0b75-3b41-4e40-aeb3-3c6143cef2dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52d2efd7-587f-4149-9b57-8b4cfbe8c9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define transforms for multi-class\n",
    "train_transform_multi = transforms.Compose([\n",
    "    #transforms.Resize(224), # DEFAULT\n",
    "    #transforms.CenterCrop(224), # DEFAULT\n",
    "    transforms.RandomResizedCrop(224, scale=(0.75, 1.0)), # Randomly crop the image to 224x224 with a scale of 75% to 100%\n",
    "    transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally with 50% probability\n",
    "    #transforms.RandomRotation(15), # Randomly rotate the image by up to +/- 15 degrees\n",
    "    #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05), # Augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform_multi = transforms.Compose([ # Minimal for test/val\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Custom Dataset wrapper to apply a specific transform\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "def setup_dataloaders(root_dir='./dataset', val_fraction=0.2, batch_size=32, num_workers=2, pin_memory=True,\n",
    "                      train_transform=None, test_transform=None, imbalanced=False, cat_breed_fraction=0.2,\n",
    "                      oversample_minority=False):\n",
    "    \"\"\"Loads data, splits, and creates DataLoaders.\"\"\"\n",
    "    # Load dataset for multi-class breed classification\n",
    "    base_train_val_dataset = OxfordIIITPet(root=root_dir, split='trainval', target_types='category', download=True)\n",
    "    test_dataset_multi_raw = OxfordIIITPet(root=root_dir, split='test', target_types='category', download=True)\n",
    "\n",
    "    # Split training data for validation\n",
    "    num_train_val_samples = len(base_train_val_dataset)\n",
    "    num_val_samples = int(val_fraction * num_train_val_samples)\n",
    "    num_train_samples_for_split = num_train_val_samples - num_val_samples\n",
    "\n",
    "    # These subsets will contain (PIL Image, label) tuples\n",
    "    train_subset_raw, val_subset_raw = random_split(base_train_val_dataset, [num_train_samples_for_split, num_val_samples])\n",
    "\n",
    "     # --- Imbalanced logic for train set ---\n",
    "    if imbalanced:\n",
    "        selection_dataset = OxfordIIITPet(root=root_dir, split='trainval', target_types=['category', 'binary-category'], download=True)\n",
    "        # Create a corresponding subset of the selection dataset\n",
    "        train_selection_subset = torch.utils.data.Subset(selection_dataset, train_subset_raw.indices)\n",
    "        imbalanced_indices = get_imbalanced_indices(train_selection_subset, cat_breed_fraction=cat_breed_fraction)\n",
    "        train_subset_multi = TransformedDataset(torch.utils.data.Subset(train_subset_raw, imbalanced_indices), transform=train_transform)\n",
    "    else:\n",
    "        train_subset_multi = TransformedDataset(train_subset_raw, transform=train_transform)\n",
    "\n",
    "    val_subset_multi = TransformedDataset(val_subset_raw, transform=test_transform)\n",
    "    test_dataset_multi = TransformedDataset(test_dataset_multi_raw, transform=test_transform)\n",
    "    \n",
    "    # --- Over-sampling logic ---\n",
    "    if oversample_minority:\n",
    "        # Get all labels from the train subset\n",
    "        labels = [y for _, y in train_subset_multi]\n",
    "        class_sample_count = np.array([np.sum(np.array(labels) == t) for t in range(len(set(labels)))])\n",
    "        class_weights = 1. / class_sample_count\n",
    "        sample_weights = np.array([class_weights[label] for label in labels])\n",
    "        sample_weights = torch.from_numpy(sample_weights).float()\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        train_loader = DataLoader(train_subset_multi, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    else:\n",
    "        # shuffle=True shuffles the data each epoch\n",
    "        train_loader = DataLoader(train_subset_multi, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    # Create DataLoader objects for the actual data subsets\n",
    "    val_loader = DataLoader(val_subset_multi, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset_multi, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"Multi-class Dataset loaded. Training samples: {len(train_subset_multi)}, Validation samples: {len(val_subset_multi)}, Test samples: {len(test_dataset_multi)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device, batchnorm_mode=\"default\"):\n",
    "    \"\"\"Trains the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Apply BatchNorm behavior based on the mode\n",
    "    if batchnorm_mode == \"freeze_params\":\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False  # Freeze gamma and beta\n",
    "    elif batchnorm_mode == \"freeze_stats\":\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                module.eval()  # Freeze running_mean and running_var\n",
    "    elif batchnorm_mode == \"default\":\n",
    "        pass  # No need to explicitly set anything; rely on PyTorch's default behavior\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels_batch in train_loader:\n",
    "        imgs, labels_batch = imgs.to(device), labels_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_epoch_loss = running_loss / len(train_loader)\n",
    "    return avg_epoch_loss\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader_to_use, criterion, device):\n",
    "    \"\"\"Evaluates the model on a given loader.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels_batch in loader_to_use:\n",
    "            imgs, labels_batch = imgs.to(device), labels_batch.to(device)\n",
    "            outputs = model(imgs)\n",
    "            if criterion: # Calculate loss if criterion is provided\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                running_loss += loss.item()\n",
    "            # For multi-class, outputs.shape will be (batch_size, 37)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels_batch).sum().item()\n",
    "            total += labels_batch.size(0)\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(loader_to_use) if criterion and len(loader_to_use) > 0 else 0.0\n",
    "    return accuracy, avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e3dca-df2f-4aa8-9991-aa2085550dd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Strategy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d995c5-986f-4fca-91fc-6eaf10a555fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_fine_tuning_strategy_1(num_epochs, lr_fc, lr_backbone, device,\n",
    "                               train_loader, val_loader, test_loader,\n",
    "                               num_classes=37, model_save_prefix=\"strategy1_best_model\",\n",
    "                               factor=0.1, patience=2, l2_lambda=0.0, batchnorm_mode=\"default\",\n",
    "                               use_weighted_loss=False):\n",
    "    \"\"\"\n",
    "    Implements Strategy 1: Fine-tune l layers simultaneously with different LRs.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting Fine-Tuning Strategy 1...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    val_accuracies_per_l = {}\n",
    "    best_overall_val_accuracy = 0.0\n",
    "    best_l_config_for_strategy = None\n",
    "    \n",
    "    max_l = 4 # For ResNet18\n",
    "\n",
    "    # --- Compute class weights if needed ---\n",
    "    weights = None\n",
    "    if use_weighted_loss:\n",
    "        # Compute class frequencies from the train_loader dataset\n",
    "        all_labels = []\n",
    "        for _, labels in train_loader.dataset:\n",
    "            all_labels.append(labels)\n",
    "        class_sample_count = np.array([np.sum(np.array(all_labels) == t) for t in range(num_classes)])\n",
    "        weight = 1. / class_sample_count\n",
    "        weights = torch.FloatTensor(weight).to(device)\n",
    "        print(f\"Using Weighted Cross-Entropy Loss. Class weights: {weights}\")\n",
    "\n",
    "    for l_val in range(1, max_l + 1):\n",
    "        print(f\"\\n    STRATEGY 1: Training with FC + last {l_val} ResNet block(s) unfrozen\")\n",
    "\n",
    "        # Initialize model for current l_val\n",
    "        current_model = resnet18(weights='IMAGENET1K_V1')\n",
    "        # Freeze all parameters initially\n",
    "        for param in current_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            # Replace the final fully connected layer (always trainable). model.fc.parameters() are requires_grad=True by default\n",
    "        current_model.fc = nn.Linear(current_model.fc.in_features, num_classes)\n",
    "        \n",
    "        # Unfreeze layers\n",
    "        current_backbone_params = []\n",
    "        if l_val >= 1: # Unfreeze layer4\n",
    "            print(\"    Unfreezing model.layer4\")\n",
    "            for param in current_model.layer4.parameters():\n",
    "                param.requires_grad = True\n",
    "                current_backbone_params.append(param)\n",
    "        if l_val >= 2: # Unfreeze layer3\n",
    "            print(\"    Unfreezing model.layer3\")\n",
    "            for param in current_model.layer3.parameters():\n",
    "                param.requires_grad = True\n",
    "                current_backbone_params.append(param)\n",
    "        if l_val >= 3: # Unfreeze layer2\n",
    "            print(\"    Unfreezing model.layer2\")\n",
    "            for param in current_model.layer2.parameters():\n",
    "                param.requires_grad = True\n",
    "                current_backbone_params.append(param)\n",
    "        if l_val >= 4: # Unfreeze layer1\n",
    "            print(\"    Unfreezing model.layer1\")\n",
    "            for param in current_model.layer1.parameters():\n",
    "                param.requires_grad = True\n",
    "                current_backbone_params.append(param)\n",
    "        \n",
    "        current_model = current_model.to(device)\n",
    "        \n",
    "        # Optimizer and Criterion for current l_val\n",
    "        if use_weighted_loss and weights is not None:\n",
    "            criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        optimizer_grouped_parameters = [{'params': current_model.fc.parameters(), 'lr': lr_fc}]\n",
    "        if current_backbone_params:\n",
    "            optimizer_grouped_parameters.append({'params': current_backbone_params, 'lr': lr_backbone})\n",
    "        \n",
    "        current_optimizer = optim.Adam(optimizer_grouped_parameters, weight_decay=l2_lambda)\n",
    "       \n",
    "        # --- Initialize ReduceLROnPlateau Scheduler ---\n",
    "        # mode='max' for accuracy, 'min' for loss.\n",
    "        # factor: Factor by which the learning rate will be reduced. new_lr = lr * factor.\n",
    "        # patience: Number of epochs with no improvement after which learning rate will be reduced.\n",
    "        # verbose=True: Prints a message when the learning rate is reduced.\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(current_optimizer, mode='max', factor=factor, patience=patience) # TUNE\n",
    "\n",
    "        total_trainable_params_in_current_model = 0\n",
    "        # Iterate over all parameters of the current_model being used for this l_val\n",
    "        for param in current_model.parameters():\n",
    "            if param.requires_grad:\n",
    "                total_trainable_params_in_current_model += param.numel()\n",
    "        print(f\"    Number of trainable parameters for l={l_val}: {total_trainable_params_in_current_model}\")\n",
    "\n",
    "        print(f\"    Starting training for l={l_val}, epochs={num_epochs}...\")\n",
    "\n",
    "        # Initialize a variable to track the previous learning rates\n",
    "        previous_lrs = None\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_train_loss = train_one_epoch(current_model, train_loader, current_optimizer, criterion, device, batchnorm_mode=batchnorm_mode)\n",
    "            \n",
    "            # Perform validation within the epoch loop for ReduceLROnPlateau\n",
    "            epoch_val_accuracy, epoch_val_loss = evaluate_model(current_model, val_loader, criterion, device)\n",
    "            \n",
    "            # Step the ReduceLROnPlateau scheduler with the validation accuracy\n",
    "            scheduler.step(epoch_val_accuracy)\n",
    "            \n",
    "            # Get the current learning rates\n",
    "            current_lrs = [group['lr'] for group in current_optimizer.param_groups]\n",
    "\n",
    "            # Print epoch details\n",
    "            print(f\"    l={l_val}, Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%, Val Loss: {epoch_val_loss:.4f}\")\n",
    "            \n",
    "            # Print the learning rates if they have changed or if this is the first epoch\n",
    "            if previous_lrs is None or current_lrs != previous_lrs:\n",
    "                print(f\"    Learning rates updated: {current_lrs}\")\n",
    "                previous_lrs = current_lrs  # Update the previous learning rates\n",
    "\n",
    "        # Perform validation for current l_val\n",
    "        print(f\"    Validating for l={l_val}...\")\n",
    "        current_l_val_accuracy, current_l_val_loss = evaluate_model(current_model, val_loader, criterion, device)\n",
    "        val_accuracies_per_l[l_val] = current_l_val_accuracy\n",
    "        print(f\"    Validation Accuracy for l={l_val}: {current_l_val_accuracy:.4f}%, Val Loss: {current_l_val_loss:.4f}\")\n",
    "\n",
    "        if current_l_val_accuracy > best_overall_val_accuracy:\n",
    "            best_overall_val_accuracy = current_l_val_accuracy\n",
    "            best_l_config_for_strategy = l_val\n",
    "            # Save the best model's state_dict\n",
    "            torch.save(current_model.state_dict(), f'{model_save_prefix}_l_{best_l_config_for_strategy}.pth')\n",
    "            print(f\"    Saved new best model (l={best_l_config_for_strategy})\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    strategy_duration_minutes = (end_time - start_time) / 60\n",
    "    \n",
    "    # After the loop, evaluate the best configuration on the test set\n",
    "    final_test_accuracy_for_strategy = 0.0\n",
    "    if best_l_config_for_strategy is not None:\n",
    "        print(f\"\\n--- Strategy 1 Evaluation ---\")\n",
    "        print(f\"Best l based on validation accuracy: {best_l_config_for_strategy} (Val Acc: {val_accuracies_per_l[best_l_config_for_strategy]:.4f}%)\")\n",
    "        print(f\"Loading and evaluating best model (l={best_l_config_for_strategy}) on the Test Set...\")\n",
    "        \n",
    "        # Re-setup the model architecture for the best_l_config\n",
    "        best_model_for_strategy = resnet18(weights=None) # Initialize without pre-trained weights if loading all\n",
    "        best_model_for_strategy.fc = nn.Linear(best_model_for_strategy.fc.in_features, num_classes) # Replace the final fully connected layer to match the saved model\n",
    "        best_model_for_strategy.load_state_dict(torch.load(f'{model_save_prefix}_l_{best_l_config_for_strategy}.pth', weights_only=True)) # Load the saved state dictionary for the best model\n",
    "        best_model_for_strategy = best_model_for_strategy.to(device)\n",
    "        \n",
    "        criterion_for_eval = nn.CrossEntropyLoss() # Re-init criterion for safety or pass it\n",
    "        final_test_accuracy_for_strategy, _ = evaluate_model(best_model_for_strategy, test_loader, criterion_for_eval, device)\n",
    "        print(f\"Final Test Accuracy (best l={best_l_config_for_strategy}): {final_test_accuracy_for_strategy:.4f}%\")\n",
    "    else:\n",
    "        print(\"No best configuration found for Strategy 1.\")\n",
    "\n",
    "    print(f\"Training time: {strategy_duration_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b47b56-73b7-4462-9f0a-cb9820df8019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Run Strategy 1 ---\n",
    "\n",
    "# These transforms are defined globally above\n",
    "actual_train_loader, actual_val_loader, actual_test_loader = setup_dataloaders(\n",
    "    train_transform=train_transform_multi,\n",
    "    test_transform=test_transform_multi,\n",
    "    batch_size=64 # TUNE\n",
    ")\n",
    "\n",
    "\n",
    "num_epochs_s1 = 15  # TUNE\n",
    "# Learning rates for FC and backbone\n",
    "lr_fc_s1 = 1e-3       # TUNE\n",
    "lr_backbone_s1 = 1e-5 # TUNE\n",
    "# Learning rate decay factor and patience for ReduceLROnPlateau\n",
    "factor = 0.1 # TUNE\n",
    "patience = 1 # TUNE\n",
    "l2_lambda = 0.0 # L2 regularization TUNE\n",
    "\n",
    "run_fine_tuning_strategy_1(\n",
    "    num_epochs=num_epochs_s1,\n",
    "    lr_fc=lr_fc_s1,\n",
    "    lr_backbone=lr_backbone_s1,\n",
    "    device=device,\n",
    "    train_loader=actual_train_loader,\n",
    "    val_loader=actual_val_loader,\n",
    "    test_loader=actual_test_loader,\n",
    "    num_classes=37,\n",
    "    model_save_prefix=\"strategy1_best_model\",\n",
    "    factor=factor, patience=patience, l2_lambda=l2_lambda,\n",
    "    batchnorm_mode=\"default\" # TUNE (\"freeze_params\", \"freeze_stats\", \"default\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19308a-15bb-4d9a-9470-c6add17ce60b",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ce032-cfd1-4a22-966b-303829e26e61",
   "metadata": {},
   "source": [
    "- **Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5909ff-ccb6-4d29-bda3-525f7e662143",
   "metadata": {
    "tags": []
   },
   "source": [
    "(num_epochs_strat1 = 20, lr_strat1 = 1e-4)\n",
    "\n",
    "--- Strategy 1 Finished ---\n",
    "Best l based on validation accuracy: 3 (Accuracy: 90.7609%)\n",
    "Loading and evaluating best model (l=3) on the Test Set...\n",
    "Final Test Accuracy (with best l=3 config): 88.6618%\n",
    "Strategy 1 training time: 12.88 minutes\n",
    "\n",
    "(num_epochs_strat1 = 20, lr_strat1 = 1e-5)\n",
    "\n",
    "--- Strategy 1 Finished ---\n",
    "Best l based on validation accuracy: 2 (Accuracy: 89.5380%)\n",
    "Loading and evaluating best model (l=2) on the Test Set...\n",
    "Final Test Accuracy (with best l=2 config): 88.2529%\n",
    "Strategy 1 training time: 17.58 minutes\n",
    "\n",
    "(num_epochs_strat1 = 15, lr_strat1 = 1e-5)\n",
    "\n",
    "--- Strategy 1 Finished ---\n",
    "Best l based on validation accuracy: 3 (Accuracy: 91.8478%)\n",
    "Loading and evaluating best model (l=3) on the Test Set...\n",
    "Final Test Accuracy (with best l=3 config): 88.0076%\n",
    "Strategy 1 training time: 12.47 minutes\n",
    "\n",
    "> (num_epochs_strat1 = 15, lr_strat1 = 1e-4)\n",
    ">\n",
    ">--- Strategy 1 Finished ---\n",
    ">Best l based on validation accuracy: 3 (Accuracy: 91.0326%)\n",
    ">Loading and evaluating best model (l=3) on the Test Set...\n",
    ">Final Test Accuracy (with best l=3 config): 89.0161%\n",
    ">Strategy 1 training time: 16.78 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4aa7a1-b8af-4826-8b63-152794590ee2",
   "metadata": {},
   "source": [
    "- **Data augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7152ea-b485-4b37-8c74-e78ca68a2ad4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Adding RandomResizedCrop** (basically the same)\n",
    "\n",
    "--- Strategy 1 Finished ---\n",
    "Best l based on validation accuracy: 1 (Accuracy: 92.5272%)\n",
    "Loading and evaluating best model (l=1) on the Test Set...\n",
    "Final Test Accuracy (with best l=1 config): 89.1251%\n",
    "Strategy 1 training time: 10.66 minutes\n",
    "\n",
    "> **Adding RandomHorizontalFlip** (almost the same)\n",
    ">\n",
    "> --- Strategy 1 Finished ---\n",
    "> Best l based on validation accuracy: 3 (Accuracy: 92.6630%)\n",
    "> Loading and evaluating best model (l=3) on the Test Set...\n",
    "> Final Test Accuracy (with best l=3 config): 89.8337%\n",
    "> Strategy 1 training time: 10.72 minutes\n",
    "\n",
    "**Adding RandomRotation** (decreases accuracy in many runs consistently) NOT ADDED\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 2 (Val Acc: 92.5272%)\n",
    "Loading and evaluating best model (l=2) on the Test Set...\n",
    "Final Test Accuracy (best l=2): 87.7351%\n",
    "Training time: 11.07 minutes\n",
    "\n",
    "**Conclusion**: RandomRotation decreased the performanced. Not added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68125ac8-facd-46f4-bc0b-119072452ebb",
   "metadata": {},
   "source": [
    "- **L2 regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20decd9e-efef-4ca2-a355-6dfa6ca839af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "l2_lambda = 1e-3\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 1 (Val Acc: 91.7120%)\n",
    "Loading and evaluating best model (l=1) on the Test Set...\n",
    "Final Test Accuracy (best l=1): 89.5067%\n",
    "Training time: 13.22 minutes\n",
    "\n",
    "\n",
    "l2_lambda = 1e-4\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 2 (Val Acc: 93.0707%)\n",
    "Loading and evaluating best model (l=2) on the Test Set...\n",
    "Final Test Accuracy (best l=2): 89.8065%\n",
    "Training time: 12.04 minutes\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 1 (Val Acc: 92.5272%)\n",
    "Loading and evaluating best model (l=1) on the Test Set...\n",
    "Final Test Accuracy (best l=1): 89.8337%\n",
    "Training time: 12.15 minutes\n",
    "\n",
    "l2_lambda = 1e-5\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 1 (Val Acc: 91.4402%)\n",
    "Loading and evaluating best model (l=1) on the Test Set...\n",
    "Final Test Accuracy (best l=1): 88.9888%\n",
    "Training time: 12.16 minutes\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 1 (Val Acc: 92.3913%)\n",
    "Loading and evaluating best model (l=1) on the Test Set...\n",
    "Final Test Accuracy (best l=1): 89.1524%\n",
    "Training time: 13.61 minutes\n",
    "\n",
    "**Conclusion**: Regularization didn't show consistent improvement. Won't be added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538ee94-7d5c-4d83-a5dc-41de1ca6d4c2",
   "metadata": {},
   "source": [
    "- **Different learning rates and learning rate schedulers for different layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86028e-409b-48ce-9718-bfdbece72bd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Batch size changed from 32 to 64.\n",
    "\n",
    "lr_fc_s1 = 1e-4 , lr_backbone_s1 = 1e-5:\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 3 (Val Acc: 91.7120%)\n",
    "Loading and evaluating best model (l=3) on the Test Set...\n",
    "Final Test Accuracy (best l=3): 90.2153%\n",
    "Training time: 14.09 minutes\n",
    "\n",
    "\n",
    "\n",
    "> lr_fc_s1 = 1e-3, lr_backbone_s1 = 1e-5:\n",
    "> \n",
    "> --- Strategy 1 Evaluation ---\n",
    "> Best l based on validation accuracy: 3 (Val Acc: 94.4293%)\n",
    "> Loading and evaluating best model (l=3) on the Test Set...\n",
    "> Final Test Accuracy (best l=3): 90.5424%\n",
    "> Training time: 17.44 minutes\n",
    ">\n",
    "> --- Strategy 1 Evaluation ---\n",
    "> Best l based on validation accuracy: 4 (Val Acc: 94.1576%)\n",
    "> Loading and evaluating best model (l=4) on the Test Set...\n",
    "> Final Test Accuracy (best l=4): 90.6241%\n",
    "> Training time: 12.65 minutes\n",
    "\n",
    "**Conclusion**: The performance improved using different learning rates and a scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e391826-a4ed-4960-a2cd-42566f37bdad",
   "metadata": {},
   "source": [
    "- **Effect of fine-tuning or not the batch-norm parameters and updating the estimate of the batch mean and standard deviations on the final performance on the new dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d31445-2352-4294-9dce-a636592ace1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Freeze batch-norm params:\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 4 (Val Acc: 92.9348%)\n",
    "Loading and evaluating best model (l=4) on the Test Set...\n",
    "Final Test Accuracy (best l=4): 90.8149%\n",
    "Training time: 12.58 minutes\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 2 (Val Acc: 93.0707%)\n",
    "Loading and evaluating best model (l=2) on the Test Set...\n",
    "Final Test Accuracy (best l=2): 89.9428%\n",
    "Training time: 12.78 minutes\n",
    "\n",
    "\n",
    "Freeze batch stats:\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 4 (Val Acc: 91.8478%)\n",
    "Loading and evaluating best model (l=4) on the Test Set...\n",
    "Final Test Accuracy (best l=4): 90.8422%\n",
    "Training time: 12.30 minutes\n",
    "\n",
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 2 (Val Acc: 91.0326%)\n",
    "Loading and evaluating best model (l=2) on the Test Set...\n",
    "Final Test Accuracy (best l=2): 90.4061%\n",
    "Training time: 12.95 minutes\n",
    "\n",
    "**Conclusion**: Validation accuracy decreased slightly when frozen, so both should be kept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e6fe6-5ed3-41f2-b34b-6be937006b30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Strategy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87723dd8-d769-41e2-b462-26a797d3dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fine_tuning_strategy_2(lr_fc, lr_backbone, device,\n",
    "                               train_loader, val_loader, test_loader,\n",
    "                               num_classes=37, model_save_prefix=\"strategy2_best_model\",\n",
    "                               factor=0.1, patience=2, l2_lambda=0.0, batchnorm_mode=\"default\",\n",
    "                               unfreeze_schedule=None):\n",
    "    \"\"\"\n",
    "    Implements Strategy 2: Gradual unfreezing of layers during fine-tuning.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting Fine-Tuning Strategy 2...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    val_accuracies_per_stage = {}\n",
    "    best_overall_val_accuracy = 0.0\n",
    "    best_stage_config = None\n",
    "\n",
    "    # Initialize model\n",
    "    current_model = resnet18(weights='IMAGENET1K_V1')\n",
    "    # Freeze all parameters initially\n",
    "    for param in current_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    current_model.fc = nn.Linear(current_model.fc.in_features, num_classes)\n",
    "    current_model = current_model.to(device)\n",
    "\n",
    "    # Optimizer and Criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Default unfreeze schedule if none is provided\n",
    "    if unfreeze_schedule is None:\n",
    "        unfreeze_schedule = [\n",
    "            {\"layers_to_unfreeze\": [\"layer4\"], \"epochs\": 5},\n",
    "            {\"layers_to_unfreeze\": [\"layer3\"], \"epochs\": 5},\n",
    "            {\"layers_to_unfreeze\": [\"layer2\"], \"epochs\": 5},\n",
    "            {\"layers_to_unfreeze\": [\"layer1\"], \"epochs\": 5},\n",
    "        ]\n",
    "\n",
    "    total_epochs = sum(stage[\"epochs\"] for stage in unfreeze_schedule)\n",
    "    current_epoch = 0\n",
    "\n",
    "    for stage_idx, stage in enumerate(unfreeze_schedule):\n",
    "        layers_to_unfreeze = stage[\"layers_to_unfreeze\"]\n",
    "        stage_epochs = stage[\"epochs\"]\n",
    "\n",
    "        print(f\"\\nStage {stage_idx + 1}: Unfreezing layers {layers_to_unfreeze} for {stage_epochs} epochs...\")\n",
    "\n",
    "        # Unfreeze specified layers\n",
    "        current_backbone_params = []\n",
    "        for layer_name in layers_to_unfreeze:\n",
    "            layer = getattr(current_model, layer_name)\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "                current_backbone_params.append(param)\n",
    "\n",
    "        # Define optimizer with updated parameter groups\n",
    "        optimizer_grouped_parameters = [{'params': current_model.fc.parameters(), 'lr': lr_fc}]\n",
    "        if current_backbone_params:\n",
    "            optimizer_grouped_parameters.append({'params': current_backbone_params, 'lr': lr_backbone})\n",
    "        current_optimizer = optim.Adam(optimizer_grouped_parameters, weight_decay=l2_lambda)\n",
    "\n",
    "        # Scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(current_optimizer, mode='max', factor=factor, patience=patience)\n",
    "\n",
    "        # Initialize a variable to track the previous learning rates\n",
    "        previous_lrs = None\n",
    "        \n",
    "        # Train for the specified number of epochs in this stage\n",
    "        for epoch in range(stage_epochs):\n",
    "            current_epoch += 1\n",
    "            avg_train_loss = train_one_epoch(current_model, train_loader, current_optimizer, criterion, device, batchnorm_mode=batchnorm_mode)\n",
    "        \n",
    "            # Perform validation\n",
    "            epoch_val_accuracy, epoch_val_loss = evaluate_model(current_model, val_loader, criterion, device)\n",
    "            scheduler.step(epoch_val_accuracy)\n",
    "        \n",
    "            # Get the current learning rates\n",
    "            current_lrs = [group['lr'] for group in current_optimizer.param_groups]\n",
    "        \n",
    "            # Print epoch details\n",
    "            print(f\"    Epoch {current_epoch}/{total_epochs} (Stage {stage_idx + 1}), Train Loss: {avg_train_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%, Val Loss: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "            # Print the learning rates only if they have changed or if this is the first epoch\n",
    "            if previous_lrs is None or current_lrs != previous_lrs:\n",
    "                print(f\"    Learning rates updated: {current_lrs}\")\n",
    "                previous_lrs = current_lrs  # Update the previous learning rates\n",
    "\n",
    "        # Validate after the stage\n",
    "        print(f\"\\nValidating after Stage {stage_idx + 1}...\")\n",
    "        stage_val_accuracy, stage_val_loss = evaluate_model(current_model, val_loader, criterion, device)\n",
    "        val_accuracies_per_stage[stage_idx + 1] = stage_val_accuracy\n",
    "        print(f\"Validation Accuracy after Stage {stage_idx + 1}: {stage_val_accuracy:.4f}%, Val Loss: {stage_val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if stage_val_accuracy > best_overall_val_accuracy:\n",
    "            best_overall_val_accuracy = stage_val_accuracy\n",
    "            best_stage_config = stage_idx + 1\n",
    "            torch.save(current_model.state_dict(), f'{model_save_prefix}_stage_{best_stage_config}.pth')\n",
    "            print(f\"    Saved new best model (Stage {best_stage_config})\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    final_test_accuracy_for_strategy = 0.0\n",
    "    if best_stage_config is not None:\n",
    "        print(f\"\\n--- Strategy 2 Evaluation ---\")\n",
    "        print(f\"Best stage based on validation accuracy: {best_stage_config} (Val Acc: {val_accuracies_per_stage[best_stage_config]:.4f}%)\")\n",
    "        print(f\"Loading and evaluating best model (Stage {best_stage_config}) on the Test Set...\")\n",
    "\n",
    "        # Re-setup the model architecture\n",
    "        best_model_for_strategy = resnet18(weights=None)\n",
    "        best_model_for_strategy.fc = nn.Linear(best_model_for_strategy.fc.in_features, num_classes)\n",
    "        best_model_for_strategy.load_state_dict(torch.load(f'{model_save_prefix}_stage_{best_stage_config}.pth', weights_only=True))\n",
    "        best_model_for_strategy = best_model_for_strategy.to(device)\n",
    "\n",
    "        criterion_for_eval = nn.CrossEntropyLoss()\n",
    "        final_test_accuracy_for_strategy, _ = evaluate_model(best_model_for_strategy, test_loader, criterion_for_eval, device)\n",
    "        print(f\"Final Test Accuracy (best stage={best_stage_config}): {final_test_accuracy_for_strategy:.4f}%\")\n",
    "    else:\n",
    "        print(\"No best configuration found for Strategy 2.\")\n",
    "\n",
    "    strategy_duration_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training time: {strategy_duration_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f4390-f749-4401-8019-b346bc558723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Run Strategy 2 ---\n",
    "\n",
    "# These transforms are defined globally above\n",
    "actual_train_loader, actual_val_loader, actual_test_loader = setup_dataloaders(\n",
    "    train_transform=train_transform_multi,\n",
    "    test_transform=test_transform_multi,\n",
    "    batch_size=64 # TUNE\n",
    ")\n",
    "\n",
    "\n",
    "unfreeze_schedule = [\n",
    "    {\"layers_to_unfreeze\": [\"layer4\"], \"epochs\": 5}, # First stage unfreezes layer4 and trains for 5 epochs\n",
    "    {\"layers_to_unfreeze\": [\"layer3\"], \"epochs\": 5}, # etc\n",
    "    {\"layers_to_unfreeze\": [\"layer2\"], \"epochs\": 5},\n",
    "    {\"layers_to_unfreeze\": [\"layer1\"], \"epochs\": 5},\n",
    "]\n",
    "\n",
    "\n",
    "run_fine_tuning_strategy_2(\n",
    "    lr_fc=1e-3, # TUNE\n",
    "    lr_backbone=1e-5, # TUNE\n",
    "    device=device,\n",
    "    train_loader=actual_train_loader,\n",
    "    val_loader=actual_val_loader,\n",
    "    test_loader=actual_test_loader,\n",
    "    num_classes=37,\n",
    "    model_save_prefix=\"strategy2_best_model\",\n",
    "    factor=0.1,\n",
    "    patience=1,\n",
    "    l2_lambda=1e-4, # TUNE 0.0\n",
    "    batchnorm_mode=\"default\",\n",
    "    unfreeze_schedule=unfreeze_schedule\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ba55d-21e6-4f7b-b2a8-25a797321267",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dca504-5a77-4121-9afb-065d8ea659a5",
   "metadata": {},
   "source": [
    "--- Strategy 2 Evaluation ---\n",
    "Best stage based on validation accuracy: 2 (Val Acc: 93.8859%)\n",
    "Loading and evaluating best model (Stage 2) on the Test Set...\n",
    "Final Test Accuracy (best stage=2): 90.0245%\n",
    "Training time: 4.15 minutes\n",
    "\n",
    "--- Strategy 2 Evaluation ---\n",
    "Best stage based on validation accuracy: 3 (Val Acc: 93.0707%)\n",
    "Loading and evaluating best model (Stage 3) on the Test Set...\n",
    "Final Test Accuracy (best stage=3): 90.2153%\n",
    "Training time: 4.87 minutes\n",
    "\n",
    "--- Strategy 2 Evaluation ---\n",
    "Best stage based on validation accuracy: 3 (Val Acc: 93.6141%)\n",
    "Loading and evaluating best model (Stage 3) on the Test Set...\n",
    "Final Test Accuracy (best stage=3): 89.6157%\n",
    "Training time: 4.83 minutes\n",
    "\n",
    "\n",
    "**Conclusion**: After fine tuning strategy 2, no better performance was achieved compared to strategy 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724dd0e-83f5-4743-8c7c-4e6840aec4b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075cb4a-506f-435a-8a34-8787d2c776f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imbalanced_indices(dataset, cat_breed_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Returns indices for an imbalanced subset: only cat_breed_fraction of each CAT breed,\n",
    "    but ALL images for dog breeds.\n",
    "    Assumes dataset returns (img, (category_label, binary_label))\n",
    "    \"\"\"\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, (_, (category_label, binary_label)) in enumerate(dataset):\n",
    "        class_to_indices[category_label].append(idx)\n",
    "    selected_indices = []\n",
    "    for category_label, indices in class_to_indices.items():\n",
    "        # Check the binary label of the first sample for this breed, since all samples of the same breed have the same binary label\n",
    "        _, first_binary_label = dataset[indices[0]][1]\n",
    "        if first_binary_label == 0:  # Cat breed\n",
    "            n_select = max(1, int(len(indices) * cat_breed_fraction))\n",
    "            selected_indices.extend(random.sample(indices, n_select))\n",
    "        else:  # Dog breed\n",
    "            selected_indices.extend(indices)\n",
    "    return selected_indices\n",
    "\n",
    "print(\"Loading imbalanced dataset...\")\n",
    "\n",
    "imbalanced_train_loader, val_loader, test_loader = setup_dataloaders(\n",
    "    train_transform=train_transform_multi,\n",
    "    test_transform=test_transform_multi,\n",
    "    batch_size=64,\n",
    "    imbalanced=True,\n",
    "    cat_breed_fraction=0.2,\n",
    "    oversample_minority=True # <--- Activate oversampling of minority classes\n",
    ")\n",
    "\n",
    "\n",
    "# Run Strategy 1 with the imbalanced train loader\n",
    "run_fine_tuning_strategy_1(\n",
    "    num_epochs=15, # TUNE\n",
    "    lr_fc=1e-3, # TUNE\n",
    "    lr_backbone=1e-5, # TUNE\n",
    "    device=device,\n",
    "    train_loader=imbalanced_train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_classes=37,\n",
    "    model_save_prefix=\"strategy1_imbalanced_model\",\n",
    "    factor=0.1, patience=1, l2_lambda=0.0, # TUNE\n",
    "    batchnorm_mode=\"default\",\n",
    "    use_weighted_loss=True  # <--- Activate weighted loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48fd0a-2199-470b-b883-96f6436d20c8",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d9d9d-7138-4d84-81b6-7f64badc35b3",
   "metadata": {},
   "source": [
    "- **Results after imbalanced classes (only 20% for each cat breed):**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "000f2bbc-2d60-461a-8e04-af9c0caab3c1",
   "metadata": {},
   "source": [
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 3 (Val Acc: 89.9457%)\n",
    "Loading and evaluating best model (l=3) on the Test Set...\n",
    "Final Test Accuracy (best l=3): 86.8629%\n",
    "Training time: 19.46 minutes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704aa66-1302-4d78-9151-1f933783719c",
   "metadata": {},
   "source": [
    "- **Weighted Cross-Entropy Loss:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "deaaa518-59b1-4693-a18a-8069a669b93d",
   "metadata": {},
   "source": [
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 4 (Val Acc: 90.4891%)\n",
    "Loading and evaluating best model (l=4) on the Test Set...\n",
    "Final Test Accuracy (best l=4): 87.1355%\n",
    "Training time: 20.42 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8822e9-8666-4d66-a033-7b9aed670d99",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Over-sampling of the minority classe**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d6cd42d-8a7f-4f8c-913e-55993cabb228",
   "metadata": {},
   "source": [
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 4 (Val Acc: 90.7609%)\n",
    "Loading and evaluating best model (l=4) on the Test Set...\n",
    "Final Test Accuracy (best l=4): 87.5170%\n",
    "Training time: 20.43 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0897a1d-6e80-4a45-ae56-ba2723922c1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Weighted Cross-Entropy Loss AND Over-sampling of the minority classe**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "944ef192-e231-4f92-8d52-0e81d105df09",
   "metadata": {},
   "source": [
    "--- Strategy 1 Evaluation ---\n",
    "Best l based on validation accuracy: 4 (Val Acc: 90.7609%)\n",
    "Loading and evaluating best model (l=4) on the Test Set...\n",
    "Final Test Accuracy (best l=4): 87.6806%\n",
    "Training time: 17.51 minutes"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
