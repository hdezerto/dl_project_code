{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d20a6c9-4d33-442d-99a2-e83610692132",
   "metadata": {},
   "source": [
    "*KTH Royal Institute of Technology* \\\n",
    "DD2424 Deep Learning in Data Science | Project (grade A part)\\\n",
    "Diogo Paulo 030224-8216 (diogop@kth.se)\\\n",
    "Hugo Dezerto 011224-8257 (hugoad@kth.se) \\\n",
    "Maria Sebasti√£o 031010-T207 (mcms2@kth.se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f10ec2b-f81e-4de0-90b5-4f86f3fbd64f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grade A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2539cedb-6e0b-426d-9043-097b29948517",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/grade_A/Diogo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to check if the current working directory is correct\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318b051-f0b4-4308-80a4-52378d5c1718",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Semi-supervised Learning (with pseudo-labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba39a24-b21e-4c1a-9912-70c9bcbbadc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Fraction of labelled data: 1.0% ==========\n",
      "\n",
      "Training with FC + last 1 ResNet block(s) unfrozen\n",
      "l=1, Epoch 1/15, Train Loss: 3.8406, Val Acc: 3.80%, Val Loss: 3.6327\n",
      "l=1, Epoch 2/15, Train Loss: 3.4986, Val Acc: 4.76%, Val Loss: 3.4970\n",
      "l=1, Epoch 3/15, Train Loss: 3.1813, Val Acc: 8.70%, Val Loss: 3.3866\n",
      "l=1, Epoch 4/15, Train Loss: 2.9126, Val Acc: 12.50%, Val Loss: 3.2886\n",
      "l=1, Epoch 5/15, Train Loss: 2.6332, Val Acc: 15.90%, Val Loss: 3.1985\n",
      "l=1, Epoch 6/15, Train Loss: 2.3488, Val Acc: 19.97%, Val Loss: 3.1134\n",
      "l=1, Epoch 7/15, Train Loss: 2.1005, Val Acc: 23.51%, Val Loss: 3.0300\n",
      "l=1, Epoch 8/15, Train Loss: 1.8672, Val Acc: 26.09%, Val Loss: 2.9484\n",
      "l=1, Epoch 9/15, Train Loss: 1.6412, Val Acc: 28.12%, Val Loss: 2.8682\n",
      "l=1, Epoch 10/15, Train Loss: 1.4232, Val Acc: 33.02%, Val Loss: 2.7896\n",
      "l=1, Epoch 11/15, Train Loss: 1.2572, Val Acc: 36.14%, Val Loss: 2.7154\n",
      "l=1, Epoch 12/15, Train Loss: 1.0125, Val Acc: 38.59%, Val Loss: 2.6446\n",
      "l=1, Epoch 13/15, Train Loss: 0.8921, Val Acc: 40.49%, Val Loss: 2.5803\n",
      "l=1, Epoch 14/15, Train Loss: 0.7053, Val Acc: 41.44%, Val Loss: 2.5207\n",
      "l=1, Epoch 15/15, Train Loss: 0.6210, Val Acc: 42.39%, Val Loss: 2.4677\n",
      "Saved new best model (l=1)\n",
      "\n",
      "Training with FC + last 2 ResNet block(s) unfrozen\n",
      "l=2, Epoch 1/15, Train Loss: 4.1116, Val Acc: 3.26%, Val Loss: 3.7170\n",
      "l=2, Epoch 2/15, Train Loss: 3.6429, Val Acc: 4.21%, Val Loss: 3.5345\n",
      "l=2, Epoch 3/15, Train Loss: 3.2934, Val Acc: 7.88%, Val Loss: 3.3966\n",
      "l=2, Epoch 4/15, Train Loss: 3.0238, Val Acc: 11.82%, Val Loss: 3.2860\n",
      "l=2, Epoch 5/15, Train Loss: 2.7195, Val Acc: 14.54%, Val Loss: 3.1898\n",
      "l=2, Epoch 6/15, Train Loss: 2.4722, Val Acc: 18.61%, Val Loss: 3.1005\n",
      "l=2, Epoch 7/15, Train Loss: 2.2221, Val Acc: 22.69%, Val Loss: 3.0152\n",
      "l=2, Epoch 8/15, Train Loss: 1.9872, Val Acc: 25.82%, Val Loss: 2.9317\n",
      "l=2, Epoch 9/15, Train Loss: 1.7273, Val Acc: 30.16%, Val Loss: 2.8525\n",
      "l=2, Epoch 10/15, Train Loss: 1.5329, Val Acc: 33.02%, Val Loss: 2.7762\n",
      "l=2, Epoch 11/15, Train Loss: 1.3437, Val Acc: 35.33%, Val Loss: 2.7031\n",
      "l=2, Epoch 12/15, Train Loss: 1.1203, Val Acc: 37.77%, Val Loss: 2.6335\n",
      "l=2, Epoch 13/15, Train Loss: 0.9569, Val Acc: 39.27%, Val Loss: 2.5698\n",
      "l=2, Epoch 14/15, Train Loss: 0.8043, Val Acc: 40.90%, Val Loss: 2.5121\n",
      "l=2, Epoch 15/15, Train Loss: 0.6795, Val Acc: 42.39%, Val Loss: 2.4589\n",
      "\n",
      "Training with FC + last 3 ResNet block(s) unfrozen\n",
      "l=3, Epoch 1/15, Train Loss: 3.7716, Val Acc: 3.26%, Val Loss: 3.6352\n",
      "l=3, Epoch 2/15, Train Loss: 3.4533, Val Acc: 4.89%, Val Loss: 3.4950\n",
      "l=3, Epoch 3/15, Train Loss: 3.0915, Val Acc: 7.74%, Val Loss: 3.3773\n",
      "l=3, Epoch 4/15, Train Loss: 2.8052, Val Acc: 13.72%, Val Loss: 3.2723\n",
      "l=3, Epoch 5/15, Train Loss: 2.5573, Val Acc: 16.71%, Val Loss: 3.1747\n",
      "l=3, Epoch 6/15, Train Loss: 2.2955, Val Acc: 21.88%, Val Loss: 3.0832\n",
      "l=3, Epoch 7/15, Train Loss: 2.0135, Val Acc: 27.17%, Val Loss: 2.9957\n",
      "l=3, Epoch 8/15, Train Loss: 1.7465, Val Acc: 29.48%, Val Loss: 2.9120\n",
      "l=3, Epoch 9/15, Train Loss: 1.5179, Val Acc: 34.24%, Val Loss: 2.8318\n",
      "l=3, Epoch 10/15, Train Loss: 1.2730, Val Acc: 37.36%, Val Loss: 2.7559\n",
      "l=3, Epoch 11/15, Train Loss: 1.1195, Val Acc: 39.27%, Val Loss: 2.6837\n",
      "l=3, Epoch 12/15, Train Loss: 0.9342, Val Acc: 40.35%, Val Loss: 2.6156\n",
      "l=3, Epoch 13/15, Train Loss: 0.7789, Val Acc: 41.30%, Val Loss: 2.5535\n",
      "l=3, Epoch 14/15, Train Loss: 0.6467, Val Acc: 41.30%, Val Loss: 2.4976\n",
      "l=3, Epoch 15/15, Train Loss: 0.5377, Val Acc: 42.26%, Val Loss: 2.4468\n",
      "\n",
      "Training with FC + last 4 ResNet block(s) unfrozen\n",
      "l=4, Epoch 1/15, Train Loss: 3.8552, Val Acc: 2.58%, Val Loss: 3.7353\n",
      "l=4, Epoch 2/15, Train Loss: 3.5247, Val Acc: 4.76%, Val Loss: 3.5950\n",
      "l=4, Epoch 3/15, Train Loss: 3.1875, Val Acc: 6.93%, Val Loss: 3.4758\n",
      "l=4, Epoch 4/15, Train Loss: 2.9085, Val Acc: 8.97%, Val Loss: 3.3705\n",
      "l=4, Epoch 5/15, Train Loss: 2.5354, Val Acc: 11.01%, Val Loss: 3.2732\n",
      "l=4, Epoch 6/15, Train Loss: 2.3370, Val Acc: 14.27%, Val Loss: 3.1805\n",
      "l=4, Epoch 7/15, Train Loss: 2.0950, Val Acc: 19.57%, Val Loss: 3.0926\n",
      "l=4, Epoch 8/15, Train Loss: 1.7712, Val Acc: 22.42%, Val Loss: 3.0076\n",
      "l=4, Epoch 9/15, Train Loss: 1.5885, Val Acc: 25.68%, Val Loss: 2.9259\n",
      "l=4, Epoch 10/15, Train Loss: 1.3769, Val Acc: 27.72%, Val Loss: 2.8482\n",
      "l=4, Epoch 11/15, Train Loss: 1.1963, Val Acc: 30.30%, Val Loss: 2.7734\n",
      "l=4, Epoch 12/15, Train Loss: 0.9948, Val Acc: 32.47%, Val Loss: 2.7054\n",
      "l=4, Epoch 13/15, Train Loss: 0.8016, Val Acc: 35.05%, Val Loss: 2.6415\n",
      "l=4, Epoch 14/15, Train Loss: 0.6544, Val Acc: 35.73%, Val Loss: 2.5835\n",
      "l=4, Epoch 15/15, Train Loss: 0.5568, Val Acc: 37.23%, Val Loss: 2.5318\n",
      "\n",
      "--- Strategy 1 Finished ---\n",
      "Best l based on validation accuracy: 1 (Accuracy: 42.3913%)\n",
      "Loading and evaluating best model (l=1) on the Test Set...\n",
      "Final Test Accuracy (with best l=1 config): 42.9000%\n",
      "Strategy 1 training time: 3.62 minutes\n",
      "\n",
      "--- Pseudo-labeling phase ---\n",
      "\n",
      "Training with FC + last 1 ResNet block(s) unfrozen\n",
      "l=1, Epoch 1/15, Train Loss: 2.5225, Val Acc: 40.90%, Val Loss: 2.1016\n",
      "l=1, Epoch 2/15, Train Loss: 1.3925, Val Acc: 48.51%, Val Loss: 1.7256\n",
      "l=1, Epoch 3/15, Train Loss: 1.0604, Val Acc: 49.59%, Val Loss: 1.6656\n",
      "l=1, Epoch 4/15, Train Loss: 0.8716, Val Acc: 52.45%, Val Loss: 1.6082\n",
      "l=1, Epoch 5/15, Train Loss: 0.7351, Val Acc: 51.90%, Val Loss: 1.6739\n",
      "l=1, Epoch 6/15, Train Loss: 0.6132, Val Acc: 50.82%, Val Loss: 1.6858\n",
      "l=1, Epoch 7/15, Train Loss: 0.5064, Val Acc: 50.82%, Val Loss: 1.6951\n",
      "l=1, Epoch 8/15, Train Loss: 0.4810, Val Acc: 50.27%, Val Loss: 1.7164\n",
      "l=1, Epoch 9/15, Train Loss: 0.4729, Val Acc: 49.59%, Val Loss: 1.7136\n",
      "l=1, Epoch 10/15, Train Loss: 0.4702, Val Acc: 49.59%, Val Loss: 1.7129\n",
      "l=1, Epoch 11/15, Train Loss: 0.4705, Val Acc: 49.86%, Val Loss: 1.7238\n",
      "l=1, Epoch 12/15, Train Loss: 0.4625, Val Acc: 50.14%, Val Loss: 1.7158\n",
      "l=1, Epoch 13/15, Train Loss: 0.4713, Val Acc: 49.73%, Val Loss: 1.7223\n",
      "l=1, Epoch 14/15, Train Loss: 0.4655, Val Acc: 49.46%, Val Loss: 1.7114\n",
      "l=1, Epoch 15/15, Train Loss: 0.4682, Val Acc: 50.27%, Val Loss: 1.7127\n",
      "Saved new best model (l=1)\n",
      "\n",
      "Training with FC + last 2 ResNet block(s) unfrozen\n",
      "l=2, Epoch 1/15, Train Loss: 2.4863, Val Acc: 41.85%, Val Loss: 1.9965\n",
      "l=2, Epoch 2/15, Train Loss: 1.3436, Val Acc: 47.55%, Val Loss: 1.7078\n",
      "l=2, Epoch 3/15, Train Loss: 1.0287, Val Acc: 47.01%, Val Loss: 1.7351\n",
      "l=2, Epoch 4/15, Train Loss: 0.8343, Val Acc: 49.86%, Val Loss: 1.6849\n",
      "l=2, Epoch 5/15, Train Loss: 0.6615, Val Acc: 49.73%, Val Loss: 1.6421\n",
      "l=2, Epoch 6/15, Train Loss: 0.5256, Val Acc: 51.90%, Val Loss: 1.7062\n",
      "l=2, Epoch 7/15, Train Loss: 0.4127, Val Acc: 49.32%, Val Loss: 1.7997\n",
      "l=2, Epoch 8/15, Train Loss: 0.3143, Val Acc: 48.78%, Val Loss: 1.8633\n",
      "l=2, Epoch 9/15, Train Loss: 0.2411, Val Acc: 49.05%, Val Loss: 1.8561\n",
      "l=2, Epoch 10/15, Train Loss: 0.2356, Val Acc: 49.46%, Val Loss: 1.8459\n",
      "l=2, Epoch 11/15, Train Loss: 0.2265, Val Acc: 49.73%, Val Loss: 1.8439\n",
      "l=2, Epoch 12/15, Train Loss: 0.2279, Val Acc: 49.05%, Val Loss: 1.8501\n",
      "l=2, Epoch 13/15, Train Loss: 0.2259, Val Acc: 48.78%, Val Loss: 1.8717\n",
      "l=2, Epoch 14/15, Train Loss: 0.2297, Val Acc: 49.32%, Val Loss: 1.8519\n",
      "l=2, Epoch 15/15, Train Loss: 0.2217, Val Acc: 49.59%, Val Loss: 1.8490\n",
      "\n",
      "Training with FC + last 3 ResNet block(s) unfrozen\n",
      "l=3, Epoch 1/15, Train Loss: 2.4901, Val Acc: 41.85%, Val Loss: 2.0361\n",
      "l=3, Epoch 2/15, Train Loss: 1.3523, Val Acc: 52.45%, Val Loss: 1.6434\n",
      "l=3, Epoch 3/15, Train Loss: 1.0157, Val Acc: 50.54%, Val Loss: 1.6793\n",
      "l=3, Epoch 4/15, Train Loss: 0.8112, Val Acc: 50.00%, Val Loss: 1.7168\n",
      "l=3, Epoch 5/15, Train Loss: 0.6581, Val Acc: 51.49%, Val Loss: 1.6584\n",
      "l=3, Epoch 6/15, Train Loss: 0.6282, Val Acc: 51.22%, Val Loss: 1.6429\n",
      "l=3, Epoch 7/15, Train Loss: 0.6125, Val Acc: 51.63%, Val Loss: 1.6411\n",
      "l=3, Epoch 8/15, Train Loss: 0.6104, Val Acc: 51.22%, Val Loss: 1.6457\n",
      "l=3, Epoch 9/15, Train Loss: 0.6081, Val Acc: 51.49%, Val Loss: 1.6498\n",
      "l=3, Epoch 10/15, Train Loss: 0.6089, Val Acc: 51.09%, Val Loss: 1.6546\n",
      "l=3, Epoch 11/15, Train Loss: 0.6073, Val Acc: 51.09%, Val Loss: 1.6494\n",
      "l=3, Epoch 12/15, Train Loss: 0.6058, Val Acc: 51.63%, Val Loss: 1.6429\n",
      "l=3, Epoch 13/15, Train Loss: 0.6116, Val Acc: 51.36%, Val Loss: 1.6446\n",
      "l=3, Epoch 14/15, Train Loss: 0.6059, Val Acc: 51.09%, Val Loss: 1.6455\n",
      "l=3, Epoch 15/15, Train Loss: 0.6132, Val Acc: 51.36%, Val Loss: 1.6476\n",
      "Saved new best model (l=3)\n",
      "\n",
      "Training with FC + last 4 ResNet block(s) unfrozen\n",
      "l=4, Epoch 1/15, Train Loss: 2.5007, Val Acc: 39.54%, Val Loss: 2.0839\n",
      "l=4, Epoch 2/15, Train Loss: 1.3395, Val Acc: 48.78%, Val Loss: 1.7338\n",
      "l=4, Epoch 3/15, Train Loss: 1.0107, Val Acc: 50.68%, Val Loss: 1.6950\n",
      "l=4, Epoch 4/15, Train Loss: 0.8068, Val Acc: 49.46%, Val Loss: 1.6796\n",
      "l=4, Epoch 5/15, Train Loss: 0.6398, Val Acc: 49.18%, Val Loss: 1.7300\n",
      "l=4, Epoch 6/15, Train Loss: 0.5003, Val Acc: 49.59%, Val Loss: 1.7213\n",
      "l=4, Epoch 7/15, Train Loss: 0.4863, Val Acc: 50.00%, Val Loss: 1.7124\n",
      "l=4, Epoch 8/15, Train Loss: 0.4758, Val Acc: 50.00%, Val Loss: 1.7216\n",
      "l=4, Epoch 9/15, Train Loss: 0.4749, Val Acc: 50.27%, Val Loss: 1.7153\n",
      "l=4, Epoch 10/15, Train Loss: 0.4647, Val Acc: 50.41%, Val Loss: 1.7211\n",
      "l=4, Epoch 11/15, Train Loss: 0.4737, Val Acc: 49.59%, Val Loss: 1.7145\n",
      "l=4, Epoch 12/15, Train Loss: 0.4673, Val Acc: 50.27%, Val Loss: 1.7192\n",
      "l=4, Epoch 13/15, Train Loss: 0.4703, Val Acc: 49.86%, Val Loss: 1.7207\n",
      "l=4, Epoch 14/15, Train Loss: 0.4641, Val Acc: 50.14%, Val Loss: 1.7192\n",
      "l=4, Epoch 15/15, Train Loss: 0.4668, Val Acc: 50.27%, Val Loss: 1.7133\n",
      "\n",
      "--- Strategy 1 Finished ---\n",
      "Best l based on validation accuracy: 3 (Accuracy: 51.3587%)\n",
      "Loading and evaluating best model (l=3) on the Test Set...\n",
      "Final Test Accuracy (with best l=3 config): 51.5127%\n",
      "\n",
      "========== Fraction of labelled data: 10.0% ==========\n",
      "\n",
      "Training with FC + last 1 ResNet block(s) unfrozen\n",
      "l=1, Epoch 1/15, Train Loss: 3.6788, Val Acc: 11.82%, Val Loss: 3.3367\n",
      "l=1, Epoch 2/15, Train Loss: 3.0213, Val Acc: 21.20%, Val Loss: 2.9697\n",
      "l=1, Epoch 3/15, Train Loss: 2.5928, Val Acc: 36.55%, Val Loss: 2.6007\n",
      "l=1, Epoch 4/15, Train Loss: 2.1573, Val Acc: 48.51%, Val Loss: 2.2809\n",
      "l=1, Epoch 5/15, Train Loss: 1.7645, Val Acc: 56.11%, Val Loss: 2.0339\n",
      "l=1, Epoch 6/15, Train Loss: 1.5630, Val Acc: 63.32%, Val Loss: 1.7927\n",
      "l=1, Epoch 7/15, Train Loss: 1.2250, Val Acc: 67.26%, Val Loss: 1.6154\n",
      "l=1, Epoch 8/15, Train Loss: 1.0330, Val Acc: 70.65%, Val Loss: 1.4698\n",
      "l=1, Epoch 9/15, Train Loss: 0.8900, Val Acc: 73.64%, Val Loss: 1.3570\n",
      "l=1, Epoch 10/15, Train Loss: 0.7385, Val Acc: 73.10%, Val Loss: 1.2877\n",
      "l=1, Epoch 11/15, Train Loss: 0.6697, Val Acc: 74.86%, Val Loss: 1.2127\n",
      "l=1, Epoch 12/15, Train Loss: 0.5552, Val Acc: 75.95%, Val Loss: 1.1479\n",
      "l=1, Epoch 13/15, Train Loss: 0.4803, Val Acc: 76.77%, Val Loss: 1.0907\n",
      "l=1, Epoch 14/15, Train Loss: 0.4472, Val Acc: 77.04%, Val Loss: 1.0415\n",
      "l=1, Epoch 15/15, Train Loss: 0.4077, Val Acc: 77.99%, Val Loss: 1.0122\n",
      "Saved new best model (l=1)\n",
      "\n",
      "Training with FC + last 2 ResNet block(s) unfrozen\n",
      "l=2, Epoch 1/15, Train Loss: 3.6032, Val Acc: 15.49%, Val Loss: 3.2332\n",
      "l=2, Epoch 2/15, Train Loss: 2.9745, Val Acc: 31.39%, Val Loss: 2.8831\n",
      "l=2, Epoch 3/15, Train Loss: 2.5115, Val Acc: 42.66%, Val Loss: 2.5242\n",
      "l=2, Epoch 4/15, Train Loss: 2.0340, Val Acc: 58.83%, Val Loss: 2.1922\n",
      "l=2, Epoch 5/15, Train Loss: 1.6704, Val Acc: 63.59%, Val Loss: 1.9499\n",
      "l=2, Epoch 6/15, Train Loss: 1.3920, Val Acc: 68.48%, Val Loss: 1.7337\n",
      "l=2, Epoch 7/15, Train Loss: 1.1329, Val Acc: 72.96%, Val Loss: 1.5590\n",
      "l=2, Epoch 8/15, Train Loss: 0.9317, Val Acc: 73.64%, Val Loss: 1.4217\n",
      "l=2, Epoch 9/15, Train Loss: 0.7758, Val Acc: 77.58%, Val Loss: 1.3000\n",
      "l=2, Epoch 10/15, Train Loss: 0.6909, Val Acc: 76.09%, Val Loss: 1.2106\n",
      "l=2, Epoch 11/15, Train Loss: 0.5680, Val Acc: 76.49%, Val Loss: 1.1358\n",
      "l=2, Epoch 12/15, Train Loss: 0.5400, Val Acc: 77.31%, Val Loss: 1.1376\n",
      "l=2, Epoch 13/15, Train Loss: 0.4800, Val Acc: 77.72%, Val Loss: 1.1314\n",
      "l=2, Epoch 14/15, Train Loss: 0.5063, Val Acc: 77.58%, Val Loss: 1.1264\n",
      "l=2, Epoch 15/15, Train Loss: 0.5107, Val Acc: 77.99%, Val Loss: 1.1193\n",
      "\n",
      "Training with FC + last 3 ResNet block(s) unfrozen\n",
      "l=3, Epoch 1/15, Train Loss: 3.6733, Val Acc: 14.81%, Val Loss: 3.2394\n",
      "l=3, Epoch 2/15, Train Loss: 2.9666, Val Acc: 31.66%, Val Loss: 2.8706\n",
      "l=3, Epoch 3/15, Train Loss: 2.5122, Val Acc: 46.33%, Val Loss: 2.5125\n",
      "l=3, Epoch 4/15, Train Loss: 2.0564, Val Acc: 54.89%, Val Loss: 2.1864\n",
      "l=3, Epoch 5/15, Train Loss: 1.6572, Val Acc: 61.96%, Val Loss: 1.9158\n",
      "l=3, Epoch 6/15, Train Loss: 1.3962, Val Acc: 66.98%, Val Loss: 1.6995\n",
      "l=3, Epoch 7/15, Train Loss: 1.1308, Val Acc: 72.42%, Val Loss: 1.5251\n",
      "l=3, Epoch 8/15, Train Loss: 0.9651, Val Acc: 74.46%, Val Loss: 1.4028\n",
      "l=3, Epoch 9/15, Train Loss: 0.7935, Val Acc: 73.37%, Val Loss: 1.3124\n",
      "l=3, Epoch 10/15, Train Loss: 0.6592, Val Acc: 73.23%, Val Loss: 1.2360\n",
      "l=3, Epoch 11/15, Train Loss: 0.5809, Val Acc: 73.51%, Val Loss: 1.2338\n",
      "l=3, Epoch 12/15, Train Loss: 0.5890, Val Acc: 74.73%, Val Loss: 1.2266\n",
      "l=3, Epoch 13/15, Train Loss: 0.5729, Val Acc: 75.95%, Val Loss: 1.2154\n",
      "l=3, Epoch 14/15, Train Loss: 0.5361, Val Acc: 76.36%, Val Loss: 1.2052\n",
      "l=3, Epoch 15/15, Train Loss: 0.5216, Val Acc: 76.77%, Val Loss: 1.1933\n",
      "\n",
      "Training with FC + last 4 ResNet block(s) unfrozen\n",
      "l=4, Epoch 1/15, Train Loss: 3.7007, Val Acc: 13.45%, Val Loss: 3.3062\n",
      "l=4, Epoch 2/15, Train Loss: 3.0439, Val Acc: 29.08%, Val Loss: 2.8967\n",
      "l=4, Epoch 3/15, Train Loss: 2.5230, Val Acc: 46.20%, Val Loss: 2.5302\n",
      "l=4, Epoch 4/15, Train Loss: 2.0899, Val Acc: 52.85%, Val Loss: 2.2380\n",
      "l=4, Epoch 5/15, Train Loss: 1.6985, Val Acc: 60.19%, Val Loss: 1.9854\n",
      "l=4, Epoch 6/15, Train Loss: 1.4079, Val Acc: 65.62%, Val Loss: 1.7716\n",
      "l=4, Epoch 7/15, Train Loss: 1.1787, Val Acc: 66.44%, Val Loss: 1.6004\n",
      "l=4, Epoch 8/15, Train Loss: 0.9296, Val Acc: 70.11%, Val Loss: 1.4578\n",
      "l=4, Epoch 9/15, Train Loss: 0.7936, Val Acc: 74.18%, Val Loss: 1.3235\n",
      "l=4, Epoch 10/15, Train Loss: 0.6757, Val Acc: 76.22%, Val Loss: 1.2201\n",
      "l=4, Epoch 11/15, Train Loss: 0.5672, Val Acc: 76.36%, Val Loss: 1.1474\n",
      "l=4, Epoch 12/15, Train Loss: 0.5063, Val Acc: 76.09%, Val Loss: 1.0901\n",
      "l=4, Epoch 13/15, Train Loss: 0.4139, Val Acc: 78.67%, Val Loss: 1.0379\n",
      "l=4, Epoch 14/15, Train Loss: 0.3835, Val Acc: 79.62%, Val Loss: 0.9989\n",
      "l=4, Epoch 15/15, Train Loss: 0.3046, Val Acc: 79.48%, Val Loss: 0.9702\n",
      "Saved new best model (l=4)\n",
      "\n",
      "--- Strategy 1 Finished ---\n",
      "Best l based on validation accuracy: 4 (Accuracy: 79.4837%)\n",
      "Loading and evaluating best model (l=4) on the Test Set...\n",
      "Final Test Accuracy (with best l=4 config): 77.8141%\n",
      "Strategy 1 training time: 4.97 minutes\n",
      "\n",
      "--- Pseudo-labeling phase ---\n",
      "\n",
      "Training with FC + last 1 ResNet block(s) unfrozen\n",
      "l=1, Epoch 1/15, Train Loss: 2.3256, Val Acc: 70.24%, Val Loss: 1.2926\n",
      "l=1, Epoch 2/15, Train Loss: 0.8625, Val Acc: 79.48%, Val Loss: 0.7888\n",
      "l=1, Epoch 3/15, Train Loss: 0.5332, Val Acc: 81.66%, Val Loss: 0.6549\n",
      "l=1, Epoch 4/15, Train Loss: 0.3883, Val Acc: 80.30%, Val Loss: 0.6236\n",
      "l=1, Epoch 5/15, Train Loss: 0.2998, Val Acc: 80.71%, Val Loss: 0.5896\n",
      "l=1, Epoch 6/15, Train Loss: 0.2419, Val Acc: 82.88%, Val Loss: 0.5699\n",
      "l=1, Epoch 7/15, Train Loss: 0.2287, Val Acc: 81.79%, Val Loss: 0.5716\n",
      "l=1, Epoch 8/15, Train Loss: 0.2274, Val Acc: 81.79%, Val Loss: 0.5664\n",
      "l=1, Epoch 9/15, Train Loss: 0.2195, Val Acc: 81.39%, Val Loss: 0.5693\n",
      "l=1, Epoch 10/15, Train Loss: 0.2165, Val Acc: 81.66%, Val Loss: 0.5680\n",
      "l=1, Epoch 11/15, Train Loss: 0.2160, Val Acc: 81.39%, Val Loss: 0.5685\n",
      "l=1, Epoch 12/15, Train Loss: 0.2120, Val Acc: 81.93%, Val Loss: 0.5669\n",
      "l=1, Epoch 13/15, Train Loss: 0.2123, Val Acc: 81.52%, Val Loss: 0.5689\n",
      "l=1, Epoch 14/15, Train Loss: 0.2140, Val Acc: 81.93%, Val Loss: 0.5669\n",
      "l=1, Epoch 15/15, Train Loss: 0.2193, Val Acc: 81.93%, Val Loss: 0.5690\n",
      "Saved new best model (l=1)\n",
      "\n",
      "Training with FC + last 2 ResNet block(s) unfrozen\n",
      "l=2, Epoch 1/15, Train Loss: 2.2624, Val Acc: 72.42%, Val Loss: 1.1935\n",
      "l=2, Epoch 2/15, Train Loss: 0.8184, Val Acc: 80.16%, Val Loss: 0.7442\n",
      "l=2, Epoch 3/15, Train Loss: 0.5080, Val Acc: 83.02%, Val Loss: 0.6062\n",
      "l=2, Epoch 4/15, Train Loss: 0.3571, Val Acc: 81.79%, Val Loss: 0.5763\n",
      "l=2, Epoch 5/15, Train Loss: 0.2739, Val Acc: 81.25%, Val Loss: 0.5753\n",
      "l=2, Epoch 6/15, Train Loss: 0.2097, Val Acc: 81.39%, Val Loss: 0.5548\n",
      "l=2, Epoch 7/15, Train Loss: 0.1964, Val Acc: 82.34%, Val Loss: 0.5515\n",
      "l=2, Epoch 8/15, Train Loss: 0.1940, Val Acc: 82.20%, Val Loss: 0.5525\n",
      "l=2, Epoch 9/15, Train Loss: 0.1888, Val Acc: 82.47%, Val Loss: 0.5492\n",
      "l=2, Epoch 10/15, Train Loss: 0.1950, Val Acc: 81.93%, Val Loss: 0.5517\n",
      "l=2, Epoch 11/15, Train Loss: 0.1913, Val Acc: 82.34%, Val Loss: 0.5494\n",
      "l=2, Epoch 12/15, Train Loss: 0.1950, Val Acc: 82.07%, Val Loss: 0.5519\n",
      "l=2, Epoch 13/15, Train Loss: 0.1886, Val Acc: 82.20%, Val Loss: 0.5512\n",
      "l=2, Epoch 14/15, Train Loss: 0.1877, Val Acc: 82.34%, Val Loss: 0.5521\n",
      "l=2, Epoch 15/15, Train Loss: 0.1899, Val Acc: 82.47%, Val Loss: 0.5547\n",
      "Saved new best model (l=2)\n",
      "\n",
      "Training with FC + last 3 ResNet block(s) unfrozen\n",
      "l=3, Epoch 1/15, Train Loss: 2.3056, Val Acc: 72.28%, Val Loss: 1.2176\n",
      "l=3, Epoch 2/15, Train Loss: 0.8156, Val Acc: 80.98%, Val Loss: 0.7345\n",
      "l=3, Epoch 3/15, Train Loss: 0.4913, Val Acc: 82.07%, Val Loss: 0.6252\n",
      "l=3, Epoch 4/15, Train Loss: 0.3363, Val Acc: 82.34%, Val Loss: 0.5898\n",
      "l=3, Epoch 5/15, Train Loss: 0.2540, Val Acc: 82.88%, Val Loss: 0.5507\n",
      "l=3, Epoch 6/15, Train Loss: 0.1834, Val Acc: 82.34%, Val Loss: 0.5518\n",
      "l=3, Epoch 7/15, Train Loss: 0.1397, Val Acc: 83.42%, Val Loss: 0.5540\n",
      "l=3, Epoch 8/15, Train Loss: 0.1057, Val Acc: 81.52%, Val Loss: 0.5640\n",
      "l=3, Epoch 9/15, Train Loss: 0.0824, Val Acc: 82.47%, Val Loss: 0.5510\n",
      "l=3, Epoch 10/15, Train Loss: 0.0663, Val Acc: 81.52%, Val Loss: 0.5581\n",
      "l=3, Epoch 11/15, Train Loss: 0.0607, Val Acc: 81.79%, Val Loss: 0.5549\n",
      "l=3, Epoch 12/15, Train Loss: 0.0596, Val Acc: 81.93%, Val Loss: 0.5599\n",
      "l=3, Epoch 13/15, Train Loss: 0.0633, Val Acc: 81.39%, Val Loss: 0.5618\n",
      "l=3, Epoch 14/15, Train Loss: 0.0626, Val Acc: 81.39%, Val Loss: 0.5606\n",
      "l=3, Epoch 15/15, Train Loss: 0.0596, Val Acc: 81.66%, Val Loss: 0.5577\n",
      "\n",
      "Training with FC + last 4 ResNet block(s) unfrozen\n",
      "l=4, Epoch 1/15, Train Loss: 2.2221, Val Acc: 74.18%, Val Loss: 1.1485\n",
      "l=4, Epoch 2/15, Train Loss: 0.7958, Val Acc: 80.98%, Val Loss: 0.7257\n",
      "l=4, Epoch 3/15, Train Loss: 0.4784, Val Acc: 83.70%, Val Loss: 0.5870\n",
      "l=4, Epoch 4/15, Train Loss: 0.3386, Val Acc: 84.24%, Val Loss: 0.5469\n",
      "l=4, Epoch 5/15, Train Loss: 0.2447, Val Acc: 81.11%, Val Loss: 0.5803\n",
      "l=4, Epoch 6/15, Train Loss: 0.1809, Val Acc: 82.61%, Val Loss: 0.5508\n",
      "l=4, Epoch 7/15, Train Loss: 0.1394, Val Acc: 83.02%, Val Loss: 0.5439\n",
      "l=4, Epoch 8/15, Train Loss: 0.1346, Val Acc: 82.88%, Val Loss: 0.5428\n",
      "l=4, Epoch 9/15, Train Loss: 0.1292, Val Acc: 83.02%, Val Loss: 0.5426\n",
      "l=4, Epoch 10/15, Train Loss: 0.1279, Val Acc: 82.88%, Val Loss: 0.5473\n",
      "l=4, Epoch 11/15, Train Loss: 0.1275, Val Acc: 83.15%, Val Loss: 0.5444\n",
      "l=4, Epoch 12/15, Train Loss: 0.1293, Val Acc: 82.74%, Val Loss: 0.5456\n",
      "l=4, Epoch 13/15, Train Loss: 0.1269, Val Acc: 82.34%, Val Loss: 0.5458\n",
      "l=4, Epoch 14/15, Train Loss: 0.1277, Val Acc: 82.88%, Val Loss: 0.5428\n",
      "l=4, Epoch 15/15, Train Loss: 0.1290, Val Acc: 83.15%, Val Loss: 0.5464\n",
      "Saved new best model (l=4)\n",
      "\n",
      "--- Strategy 1 Finished ---\n",
      "Best l based on validation accuracy: 4 (Accuracy: 83.1522%)\n",
      "Loading and evaluating best model (l=4) on the Test Set...\n",
      "Final Test Accuracy (with best l=4 config): 81.4391%\n",
      "\n",
      "========== Fraction of labelled data: 50.0% ==========\n",
      "\n",
      "Training with FC + last 1 ResNet block(s) unfrozen\n",
      "l=1, Epoch 1/15, Train Loss: 2.9621, Val Acc: 57.74%, Val Loss: 2.0461\n",
      "l=1, Epoch 2/15, Train Loss: 1.5420, Val Acc: 79.76%, Val Loss: 1.1530\n",
      "l=1, Epoch 3/15, Train Loss: 0.9227, Val Acc: 85.05%, Val Loss: 0.8218\n",
      "l=1, Epoch 4/15, Train Loss: 0.6435, Val Acc: 88.18%, Val Loss: 0.6359\n",
      "l=1, Epoch 5/15, Train Loss: 0.4986, Val Acc: 88.86%, Val Loss: 0.5517\n",
      "l=1, Epoch 6/15, Train Loss: 0.4062, Val Acc: 89.95%, Val Loss: 0.4801\n",
      "l=1, Epoch 7/15, Train Loss: 0.3317, Val Acc: 88.99%, Val Loss: 0.4514\n",
      "l=1, Epoch 8/15, Train Loss: 0.2733, Val Acc: 89.81%, Val Loss: 0.4149\n",
      "l=1, Epoch 9/15, Train Loss: 0.2417, Val Acc: 90.22%, Val Loss: 0.4084\n",
      "l=1, Epoch 10/15, Train Loss: 0.2396, Val Acc: 89.95%, Val Loss: 0.4012\n",
      "l=1, Epoch 11/15, Train Loss: 0.2338, Val Acc: 89.95%, Val Loss: 0.3959\n",
      "l=1, Epoch 12/15, Train Loss: 0.2321, Val Acc: 90.22%, Val Loss: 0.3949\n",
      "l=1, Epoch 13/15, Train Loss: 0.2319, Val Acc: 90.35%, Val Loss: 0.3955\n",
      "l=1, Epoch 14/15, Train Loss: 0.2265, Val Acc: 90.76%, Val Loss: 0.3976\n",
      "l=1, Epoch 15/15, Train Loss: 0.2285, Val Acc: 90.08%, Val Loss: 0.3966\n",
      "Saved new best model (l=1)\n",
      "\n",
      "Training with FC + last 2 ResNet block(s) unfrozen\n",
      "l=2, Epoch 1/15, Train Loss: 2.9251, Val Acc: 59.10%, Val Loss: 2.0406\n",
      "l=2, Epoch 2/15, Train Loss: 1.4990, Val Acc: 78.94%, Val Loss: 1.1184\n",
      "l=2, Epoch 3/15, Train Loss: 0.8849, Val Acc: 84.38%, Val Loss: 0.7802\n",
      "l=2, Epoch 4/15, Train Loss: 0.6098, Val Acc: 87.09%, Val Loss: 0.6129\n",
      "l=2, Epoch 5/15, Train Loss: 0.4625, Val Acc: 88.72%, Val Loss: 0.5244\n",
      "l=2, Epoch 6/15, Train Loss: 0.3691, Val Acc: 88.72%, Val Loss: 0.4645\n",
      "l=2, Epoch 7/15, Train Loss: 0.3030, Val Acc: 89.27%, Val Loss: 0.4295\n",
      "l=2, Epoch 8/15, Train Loss: 0.2506, Val Acc: 90.49%, Val Loss: 0.3894\n",
      "l=2, Epoch 9/15, Train Loss: 0.2139, Val Acc: 90.49%, Val Loss: 0.3735\n",
      "l=2, Epoch 10/15, Train Loss: 0.1786, Val Acc: 90.62%, Val Loss: 0.3619\n",
      "l=2, Epoch 11/15, Train Loss: 0.1497, Val Acc: 90.35%, Val Loss: 0.3363\n",
      "l=2, Epoch 12/15, Train Loss: 0.1316, Val Acc: 90.90%, Val Loss: 0.3325\n",
      "l=2, Epoch 13/15, Train Loss: 0.1149, Val Acc: 90.22%, Val Loss: 0.3208\n",
      "l=2, Epoch 14/15, Train Loss: 0.0989, Val Acc: 90.90%, Val Loss: 0.3162\n",
      "l=2, Epoch 15/15, Train Loss: 0.0880, Val Acc: 90.90%, Val Loss: 0.3143\n",
      "Saved new best model (l=2)\n",
      "\n",
      "Training with FC + last 3 ResNet block(s) unfrozen\n",
      "l=3, Epoch 1/15, Train Loss: 2.9180, Val Acc: 59.65%, Val Loss: 2.0046\n",
      "l=3, Epoch 2/15, Train Loss: 1.4717, Val Acc: 81.25%, Val Loss: 1.0815\n",
      "l=3, Epoch 3/15, Train Loss: 0.8414, Val Acc: 87.09%, Val Loss: 0.7326\n",
      "l=3, Epoch 4/15, Train Loss: 0.5756, Val Acc: 88.86%, Val Loss: 0.5889\n",
      "l=3, Epoch 5/15, Train Loss: 0.4299, Val Acc: 89.54%, Val Loss: 0.5030\n",
      "l=3, Epoch 6/15, Train Loss: 0.3517, Val Acc: 91.71%, Val Loss: 0.4439\n",
      "l=3, Epoch 7/15, Train Loss: 0.2836, Val Acc: 90.08%, Val Loss: 0.4181\n",
      "l=3, Epoch 8/15, Train Loss: 0.2439, Val Acc: 91.03%, Val Loss: 0.3771\n",
      "l=3, Epoch 9/15, Train Loss: 0.2025, Val Acc: 91.85%, Val Loss: 0.3669\n",
      "l=3, Epoch 10/15, Train Loss: 0.1964, Val Acc: 91.58%, Val Loss: 0.3651\n",
      "l=3, Epoch 11/15, Train Loss: 0.1848, Val Acc: 91.71%, Val Loss: 0.3624\n",
      "l=3, Epoch 12/15, Train Loss: 0.1883, Val Acc: 91.58%, Val Loss: 0.3620\n",
      "l=3, Epoch 13/15, Train Loss: 0.1814, Val Acc: 91.58%, Val Loss: 0.3632\n",
      "l=3, Epoch 14/15, Train Loss: 0.1877, Val Acc: 91.58%, Val Loss: 0.3612\n",
      "l=3, Epoch 15/15, Train Loss: 0.1958, Val Acc: 91.30%, Val Loss: 0.3622\n",
      "Saved new best model (l=3)\n",
      "\n",
      "Training with FC + last 4 ResNet block(s) unfrozen\n",
      "l=4, Epoch 1/15, Train Loss: 3.0098, Val Acc: 61.14%, Val Loss: 2.0636\n",
      "l=4, Epoch 2/15, Train Loss: 1.5456, Val Acc: 78.40%, Val Loss: 1.1782\n",
      "l=4, Epoch 3/15, Train Loss: 0.8935, Val Acc: 86.14%, Val Loss: 0.7786\n",
      "l=4, Epoch 4/15, Train Loss: 0.6176, Val Acc: 87.50%, Val Loss: 0.6134\n",
      "l=4, Epoch 5/15, Train Loss: 0.4596, Val Acc: 88.86%, Val Loss: 0.5260\n",
      "l=4, Epoch 6/15, Train Loss: 0.3597, Val Acc: 90.49%, Val Loss: 0.4520\n",
      "l=4, Epoch 7/15, Train Loss: 0.2895, Val Acc: 90.90%, Val Loss: 0.4191\n",
      "l=4, Epoch 8/15, Train Loss: 0.2448, Val Acc: 91.03%, Val Loss: 0.3837\n",
      "l=4, Epoch 9/15, Train Loss: 0.1971, Val Acc: 91.58%, Val Loss: 0.3608\n",
      "l=4, Epoch 10/15, Train Loss: 0.1713, Val Acc: 91.17%, Val Loss: 0.3430\n",
      "l=4, Epoch 11/15, Train Loss: 0.1455, Val Acc: 91.17%, Val Loss: 0.3318\n",
      "l=4, Epoch 12/15, Train Loss: 0.1236, Val Acc: 91.17%, Val Loss: 0.3266\n",
      "l=4, Epoch 13/15, Train Loss: 0.1278, Val Acc: 91.30%, Val Loss: 0.3237\n",
      "l=4, Epoch 14/15, Train Loss: 0.1236, Val Acc: 91.30%, Val Loss: 0.3235\n",
      "l=4, Epoch 15/15, Train Loss: 0.1186, Val Acc: 91.71%, Val Loss: 0.3226\n",
      "Saved new best model (l=4)\n",
      "\n",
      "--- Strategy 1 Finished ---\n",
      "Best l based on validation accuracy: 4 (Accuracy: 91.7120%)\n",
      "Loading and evaluating best model (l=4) on the Test Set...\n",
      "Final Test Accuracy (with best l=4 config): 89.0161%\n",
      "Strategy 1 training time: 11.19 minutes\n",
      "\n",
      "--- Pseudo-labeling phase ---\n",
      "\n",
      "Training with FC + last 1 ResNet block(s) unfrozen\n",
      "l=1, Epoch 1/15, Train Loss: 2.2862, Val Acc: 78.12%, Val Loss: 1.1391\n",
      "l=1, Epoch 2/15, Train Loss: 0.7999, Val Acc: 86.96%, Val Loss: 0.6382\n",
      "l=1, Epoch 3/15, Train Loss: 0.4631, Val Acc: 89.40%, Val Loss: 0.4699\n",
      "l=1, Epoch 4/15, Train Loss: 0.3342, Val Acc: 90.35%, Val Loss: 0.4113\n",
      "l=1, Epoch 5/15, Train Loss: 0.2512, Val Acc: 91.30%, Val Loss: 0.3549\n",
      "l=1, Epoch 6/15, Train Loss: 0.1998, Val Acc: 91.30%, Val Loss: 0.3245\n",
      "l=1, Epoch 7/15, Train Loss: 0.1621, Val Acc: 91.44%, Val Loss: 0.3047\n",
      "l=1, Epoch 8/15, Train Loss: 0.1335, Val Acc: 91.44%, Val Loss: 0.3003\n",
      "l=1, Epoch 9/15, Train Loss: 0.1087, Val Acc: 91.03%, Val Loss: 0.2944\n",
      "l=1, Epoch 10/15, Train Loss: 0.0959, Val Acc: 91.58%, Val Loss: 0.2921\n",
      "l=1, Epoch 11/15, Train Loss: 0.0876, Val Acc: 91.85%, Val Loss: 0.2859\n",
      "l=1, Epoch 12/15, Train Loss: 0.0863, Val Acc: 91.44%, Val Loss: 0.2833\n",
      "l=1, Epoch 13/15, Train Loss: 0.0882, Val Acc: 91.85%, Val Loss: 0.2839\n",
      "l=1, Epoch 14/15, Train Loss: 0.0849, Val Acc: 91.71%, Val Loss: 0.2840\n",
      "l=1, Epoch 15/15, Train Loss: 0.0819, Val Acc: 91.71%, Val Loss: 0.2852\n",
      "Saved new best model (l=1)\n",
      "\n",
      "Training with FC + last 2 ResNet block(s) unfrozen\n",
      "l=2, Epoch 1/15, Train Loss: 2.2715, Val Acc: 79.35%, Val Loss: 1.1157\n",
      "l=2, Epoch 2/15, Train Loss: 0.7766, Val Acc: 89.81%, Val Loss: 0.5922\n",
      "l=2, Epoch 3/15, Train Loss: 0.4417, Val Acc: 90.62%, Val Loss: 0.4459\n",
      "l=2, Epoch 4/15, Train Loss: 0.3120, Val Acc: 91.44%, Val Loss: 0.3724\n",
      "l=2, Epoch 5/15, Train Loss: 0.2302, Val Acc: 91.98%, Val Loss: 0.3255\n",
      "l=2, Epoch 6/15, Train Loss: 0.1801, Val Acc: 92.26%, Val Loss: 0.3026\n",
      "l=2, Epoch 7/15, Train Loss: 0.1391, Val Acc: 92.12%, Val Loss: 0.2859\n",
      "l=2, Epoch 8/15, Train Loss: 0.1142, Val Acc: 92.12%, Val Loss: 0.2834\n",
      "l=2, Epoch 9/15, Train Loss: 0.0937, Val Acc: 91.98%, Val Loss: 0.2781\n",
      "l=2, Epoch 10/15, Train Loss: 0.0931, Val Acc: 91.71%, Val Loss: 0.2756\n",
      "l=2, Epoch 11/15, Train Loss: 0.0854, Val Acc: 91.58%, Val Loss: 0.2760\n",
      "l=2, Epoch 12/15, Train Loss: 0.0880, Val Acc: 91.71%, Val Loss: 0.2755\n",
      "l=2, Epoch 13/15, Train Loss: 0.0865, Val Acc: 91.44%, Val Loss: 0.2747\n",
      "l=2, Epoch 14/15, Train Loss: 0.0892, Val Acc: 91.58%, Val Loss: 0.2753\n",
      "l=2, Epoch 15/15, Train Loss: 0.0867, Val Acc: 91.71%, Val Loss: 0.2747\n",
      "\n",
      "Training with FC + last 3 ResNet block(s) unfrozen\n",
      "l=3, Epoch 1/15, Train Loss: 2.2755, Val Acc: 80.16%, Val Loss: 1.0893\n",
      "l=3, Epoch 2/15, Train Loss: 0.7543, Val Acc: 86.41%, Val Loss: 0.5783\n",
      "l=3, Epoch 3/15, Train Loss: 0.4253, Val Acc: 89.40%, Val Loss: 0.4322\n",
      "l=3, Epoch 4/15, Train Loss: 0.2940, Val Acc: 90.08%, Val Loss: 0.3745\n",
      "l=3, Epoch 5/15, Train Loss: 0.2204, Val Acc: 90.76%, Val Loss: 0.3331\n",
      "l=3, Epoch 6/15, Train Loss: 0.1694, Val Acc: 90.62%, Val Loss: 0.3169\n",
      "l=3, Epoch 7/15, Train Loss: 0.1330, Val Acc: 90.49%, Val Loss: 0.2970\n",
      "l=3, Epoch 8/15, Train Loss: 0.1035, Val Acc: 91.17%, Val Loss: 0.2873\n",
      "l=3, Epoch 9/15, Train Loss: 0.1035, Val Acc: 91.17%, Val Loss: 0.2895\n",
      "l=3, Epoch 10/15, Train Loss: 0.0991, Val Acc: 91.03%, Val Loss: 0.2848\n",
      "l=3, Epoch 11/15, Train Loss: 0.0959, Val Acc: 90.90%, Val Loss: 0.2863\n",
      "l=3, Epoch 12/15, Train Loss: 0.0960, Val Acc: 91.17%, Val Loss: 0.2820\n",
      "l=3, Epoch 13/15, Train Loss: 0.0972, Val Acc: 90.76%, Val Loss: 0.2858\n",
      "l=3, Epoch 14/15, Train Loss: 0.0953, Val Acc: 90.90%, Val Loss: 0.2850\n",
      "l=3, Epoch 15/15, Train Loss: 0.0954, Val Acc: 91.03%, Val Loss: 0.2850\n",
      "\n",
      "Training with FC + last 4 ResNet block(s) unfrozen\n",
      "l=4, Epoch 1/15, Train Loss: 2.2801, Val Acc: 80.98%, Val Loss: 1.0840\n",
      "l=4, Epoch 2/15, Train Loss: 0.7591, Val Acc: 87.64%, Val Loss: 0.5758\n",
      "l=4, Epoch 3/15, Train Loss: 0.4248, Val Acc: 88.72%, Val Loss: 0.4405\n",
      "l=4, Epoch 4/15, Train Loss: 0.2945, Val Acc: 90.90%, Val Loss: 0.3753\n",
      "l=4, Epoch 5/15, Train Loss: 0.2218, Val Acc: 90.08%, Val Loss: 0.3485\n",
      "l=4, Epoch 6/15, Train Loss: 0.1588, Val Acc: 91.03%, Val Loss: 0.3043\n",
      "l=4, Epoch 7/15, Train Loss: 0.1307, Val Acc: 91.30%, Val Loss: 0.2905\n",
      "l=4, Epoch 8/15, Train Loss: 0.1058, Val Acc: 91.17%, Val Loss: 0.2824\n",
      "l=4, Epoch 9/15, Train Loss: 0.0840, Val Acc: 91.44%, Val Loss: 0.2801\n",
      "l=4, Epoch 10/15, Train Loss: 0.0693, Val Acc: 91.17%, Val Loss: 0.2692\n",
      "l=4, Epoch 11/15, Train Loss: 0.0575, Val Acc: 91.17%, Val Loss: 0.2770\n",
      "l=4, Epoch 12/15, Train Loss: 0.0473, Val Acc: 91.03%, Val Loss: 0.2709\n",
      "l=4, Epoch 13/15, Train Loss: 0.0436, Val Acc: 91.17%, Val Loss: 0.2682\n",
      "l=4, Epoch 14/15, Train Loss: 0.0422, Val Acc: 91.17%, Val Loss: 0.2670\n",
      "l=4, Epoch 15/15, Train Loss: 0.0427, Val Acc: 91.30%, Val Loss: 0.2684\n",
      "\n",
      "--- Strategy 1 Finished ---\n",
      "Best l based on validation accuracy: 1 (Accuracy: 91.7120%)\n",
      "Loading and evaluating best model (l=1) on the Test Set...\n",
      "Final Test Accuracy (with best l=1 config): 88.9616%\n",
      "\n",
      "========== Fraction of labelled data: 100.0% ==========\n",
      "\n",
      "Training with FC + last 1 ResNet block(s) unfrozen\n",
      "l=1, Epoch 1/15, Train Loss: 2.3094, Val Acc: 80.03%, Val Loss: 1.1542\n",
      "l=1, Epoch 2/15, Train Loss: 0.8518, Val Acc: 87.64%, Val Loss: 0.6381\n",
      "l=1, Epoch 3/15, Train Loss: 0.5289, Val Acc: 90.35%, Val Loss: 0.4729\n",
      "l=1, Epoch 4/15, Train Loss: 0.3890, Val Acc: 90.90%, Val Loss: 0.3935\n",
      "l=1, Epoch 5/15, Train Loss: 0.3093, Val Acc: 91.17%, Val Loss: 0.3521\n",
      "l=1, Epoch 6/15, Train Loss: 0.2561, Val Acc: 91.17%, Val Loss: 0.3274\n",
      "l=1, Epoch 7/15, Train Loss: 0.2126, Val Acc: 91.71%, Val Loss: 0.3032\n",
      "l=1, Epoch 8/15, Train Loss: 0.1795, Val Acc: 92.66%, Val Loss: 0.2864\n",
      "l=1, Epoch 9/15, Train Loss: 0.1448, Val Acc: 91.98%, Val Loss: 0.2738\n",
      "l=1, Epoch 10/15, Train Loss: 0.1282, Val Acc: 92.66%, Val Loss: 0.2655\n",
      "l=1, Epoch 11/15, Train Loss: 0.1099, Val Acc: 92.80%, Val Loss: 0.2591\n",
      "l=1, Epoch 12/15, Train Loss: 0.1056, Val Acc: 92.39%, Val Loss: 0.2566\n",
      "l=1, Epoch 13/15, Train Loss: 0.1001, Val Acc: 92.66%, Val Loss: 0.2566\n",
      "l=1, Epoch 14/15, Train Loss: 0.1061, Val Acc: 92.53%, Val Loss: 0.2551\n",
      "l=1, Epoch 15/15, Train Loss: 0.1013, Val Acc: 92.80%, Val Loss: 0.2562\n",
      "Saved new best model (l=1)\n",
      "\n",
      "Training with FC + last 2 ResNet block(s) unfrozen\n",
      "l=2, Epoch 1/15, Train Loss: 2.3382, Val Acc: 81.11%, Val Loss: 1.1285\n",
      "l=2, Epoch 2/15, Train Loss: 0.8406, Val Acc: 87.23%, Val Loss: 0.6115\n",
      "l=2, Epoch 3/15, Train Loss: 0.4971, Val Acc: 88.99%, Val Loss: 0.4560\n",
      "l=2, Epoch 4/15, Train Loss: 0.3560, Val Acc: 91.17%, Val Loss: 0.3684\n",
      "l=2, Epoch 5/15, Train Loss: 0.2765, Val Acc: 91.17%, Val Loss: 0.3253\n",
      "l=2, Epoch 6/15, Train Loss: 0.2255, Val Acc: 91.58%, Val Loss: 0.2970\n",
      "l=2, Epoch 7/15, Train Loss: 0.1878, Val Acc: 93.07%, Val Loss: 0.2672\n",
      "l=2, Epoch 8/15, Train Loss: 0.1611, Val Acc: 92.12%, Val Loss: 0.2598\n",
      "l=2, Epoch 9/15, Train Loss: 0.1352, Val Acc: 92.12%, Val Loss: 0.2529\n",
      "l=2, Epoch 10/15, Train Loss: 0.1016, Val Acc: 92.53%, Val Loss: 0.2462\n",
      "l=2, Epoch 11/15, Train Loss: 0.0981, Val Acc: 92.80%, Val Loss: 0.2456\n",
      "l=2, Epoch 12/15, Train Loss: 0.0997, Val Acc: 92.80%, Val Loss: 0.2478\n",
      "l=2, Epoch 13/15, Train Loss: 0.1021, Val Acc: 92.39%, Val Loss: 0.2472\n",
      "l=2, Epoch 14/15, Train Loss: 0.0977, Val Acc: 93.07%, Val Loss: 0.2435\n",
      "l=2, Epoch 15/15, Train Loss: 0.0994, Val Acc: 92.66%, Val Loss: 0.2459\n",
      "\n",
      "Training with FC + last 3 ResNet block(s) unfrozen\n",
      "l=3, Epoch 1/15, Train Loss: 2.2735, Val Acc: 80.71%, Val Loss: 1.0762\n",
      "l=3, Epoch 2/15, Train Loss: 0.7971, Val Acc: 89.13%, Val Loss: 0.5799\n",
      "l=3, Epoch 3/15, Train Loss: 0.4767, Val Acc: 90.35%, Val Loss: 0.4382\n",
      "l=3, Epoch 4/15, Train Loss: 0.3440, Val Acc: 91.44%, Val Loss: 0.3585\n",
      "l=3, Epoch 5/15, Train Loss: 0.2618, Val Acc: 91.98%, Val Loss: 0.3213\n",
      "l=3, Epoch 6/15, Train Loss: 0.2171, Val Acc: 93.07%, Val Loss: 0.2791\n",
      "l=3, Epoch 7/15, Train Loss: 0.1746, Val Acc: 92.66%, Val Loss: 0.2774\n",
      "l=3, Epoch 8/15, Train Loss: 0.1442, Val Acc: 93.21%, Val Loss: 0.2590\n",
      "l=3, Epoch 9/15, Train Loss: 0.1142, Val Acc: 93.48%, Val Loss: 0.2443\n",
      "l=3, Epoch 10/15, Train Loss: 0.0982, Val Acc: 92.80%, Val Loss: 0.2328\n",
      "l=3, Epoch 11/15, Train Loss: 0.0842, Val Acc: 92.53%, Val Loss: 0.2341\n",
      "l=3, Epoch 12/15, Train Loss: 0.0666, Val Acc: 93.48%, Val Loss: 0.2284\n",
      "l=3, Epoch 13/15, Train Loss: 0.0663, Val Acc: 93.34%, Val Loss: 0.2284\n",
      "l=3, Epoch 14/15, Train Loss: 0.0623, Val Acc: 93.61%, Val Loss: 0.2284\n",
      "l=3, Epoch 15/15, Train Loss: 0.0632, Val Acc: 93.07%, Val Loss: 0.2290\n",
      "Saved new best model (l=3)\n",
      "\n",
      "Training with FC + last 4 ResNet block(s) unfrozen\n",
      "l=4, Epoch 1/15, Train Loss: 2.2368, Val Acc: 80.16%, Val Loss: 1.0814\n",
      "l=4, Epoch 2/15, Train Loss: 0.7861, Val Acc: 88.72%, Val Loss: 0.5741\n",
      "l=4, Epoch 3/15, Train Loss: 0.4675, Val Acc: 89.27%, Val Loss: 0.4377\n",
      "l=4, Epoch 4/15, Train Loss: 0.3306, Val Acc: 90.08%, Val Loss: 0.3619\n",
      "l=4, Epoch 5/15, Train Loss: 0.2567, Val Acc: 91.85%, Val Loss: 0.3090\n",
      "l=4, Epoch 6/15, Train Loss: 0.2018, Val Acc: 92.12%, Val Loss: 0.2969\n",
      "l=4, Epoch 7/15, Train Loss: 0.1756, Val Acc: 92.12%, Val Loss: 0.2715\n",
      "l=4, Epoch 8/15, Train Loss: 0.1306, Val Acc: 92.26%, Val Loss: 0.2586\n",
      "l=4, Epoch 9/15, Train Loss: 0.1114, Val Acc: 92.53%, Val Loss: 0.2467\n",
      "l=4, Epoch 10/15, Train Loss: 0.0887, Val Acc: 92.53%, Val Loss: 0.2422\n",
      "l=4, Epoch 11/15, Train Loss: 0.0760, Val Acc: 92.39%, Val Loss: 0.2381\n",
      "l=4, Epoch 12/15, Train Loss: 0.0670, Val Acc: 92.93%, Val Loss: 0.2333\n",
      "l=4, Epoch 13/15, Train Loss: 0.0626, Val Acc: 92.80%, Val Loss: 0.2323\n",
      "l=4, Epoch 14/15, Train Loss: 0.0626, Val Acc: 92.80%, Val Loss: 0.2301\n",
      "l=4, Epoch 15/15, Train Loss: 0.0641, Val Acc: 93.07%, Val Loss: 0.2303\n",
      "\n",
      "--- Strategy 1 Finished ---\n",
      "Best l based on validation accuracy: 3 (Accuracy: 93.0707%)\n",
      "Loading and evaluating best model (l=3) on the Test Set...\n",
      "Final Test Accuracy (with best l=3 config): 90.1608%\n",
      "Strategy 1 training time: 24.89 minutes\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# --- Config ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 37\n",
    "VAL_FRACTION = 0.2\n",
    "NUM_EPOCHS = 15\n",
    "LR_FC = 1e-3\n",
    "LR_BACKBONE = 1e-5\n",
    "FACTOR = 0.1\n",
    "PATIENCE = 1\n",
    "L2_LAMBDA = 0.0\n",
    "MODEL_PREFIX = \"semi_supervised_strategy1\"\n",
    "FRACTIONS = [0.01, 0.1, 0.5, 1.0]  # Labelled data fractions to test\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.75, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Dataset Wrappers ---\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "class PseudoLabelledDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx].item()\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def subsample_dataset(dataset, fraction, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    targets = np.array([y for _, y in dataset])\n",
    "    classes = np.unique(targets)\n",
    "    indices = []\n",
    "    for c in classes:\n",
    "        class_indices = np.where(targets == c)[0]\n",
    "        n_samples = max(1, int(len(class_indices) * fraction))\n",
    "        indices.extend(np.random.choice(class_indices, n_samples, replace=False))\n",
    "    np.random.shuffle(indices)\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "def generate_pseudo_labels(model, unlabelled_loader):\n",
    "    model.eval()\n",
    "    pseudo_images = []\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in unlabelled_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            pseudo_images.append(imgs.cpu())\n",
    "            pseudo_labels.append(preds.cpu())\n",
    "    if pseudo_images:\n",
    "        pseudo_images = torch.cat(pseudo_images)\n",
    "        pseudo_labels = torch.cat(pseudo_labels)\n",
    "        return PseudoLabelledDataset(pseudo_images, pseudo_labels)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def evaluate_model(model, loader, criterion=None):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            if criterion:\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(loader) if criterion else 0.0\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def get_unfrozen_params(model, l):\n",
    "    # Unfreeze last l ResNet blocks (layer4, layer3, layer2, layer1)\n",
    "    backbone_params = []\n",
    "    if l >= 1:\n",
    "        for param in model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "            backbone_params.append(param)\n",
    "    if l >= 2:\n",
    "        for param in model.layer3.parameters():\n",
    "            param.requires_grad = True\n",
    "            backbone_params.append(param)\n",
    "    if l >= 3:\n",
    "        for param in model.layer2.parameters():\n",
    "            param.requires_grad = True\n",
    "            backbone_params.append(param)\n",
    "    if l >= 4:\n",
    "        for param in model.layer1.parameters():\n",
    "            param.requires_grad = True\n",
    "            backbone_params.append(param)\n",
    "    return backbone_params\n",
    "\n",
    "# From Grade E\n",
    "def run_strategy1(train_loader, val_loader, test_loader, num_classes, num_epochs, lr_fc, lr_backbone, factor, patience, l2_lambda, model_prefix):\n",
    "    best_val_acc = 0.0\n",
    "    best_l = None\n",
    "    val_accuracies_per_l = {}\n",
    "    max_l = 4\n",
    "    for l in range(1, max_l + 1):\n",
    "        print(f\"\\nTraining with FC + last {l} ResNet block(s) unfrozen\")\n",
    "        model = resnet18(weights='IMAGENET1K_V1')\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        backbone_params = get_unfrozen_params(model, l)\n",
    "        model = model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': model.fc.parameters(), 'lr': lr_fc}\n",
    "        ]\n",
    "        if backbone_params:\n",
    "            optimizer_grouped_parameters.append({'params': backbone_params, 'lr': lr_backbone})\n",
    "        optimizer = optim.Adam(optimizer_grouped_parameters, weight_decay=l2_lambda)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=factor, patience=patience)\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "            val_acc, val_loss = evaluate_model(model, val_loader, criterion)\n",
    "            scheduler.step(val_acc)\n",
    "            print(f\"l={l}, Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Acc: {val_acc:.2f}%, Val Loss: {val_loss:.4f}\")\n",
    "        val_acc, _ = evaluate_model(model, val_loader, criterion)\n",
    "        val_accuracies_per_l[l] = val_acc\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_l = l\n",
    "            torch.save(model.state_dict(), f\"{model_prefix}_l_{l}.pth\")\n",
    "            print(f\"Saved new best model (l={l})\")\n",
    "    # Evaluate best model on test set\n",
    "    print(f\"\\n--- Strategy 1 Finished ---\")\n",
    "    print(f\"Best l based on validation accuracy: {best_l} (Accuracy: {val_accuracies_per_l[best_l]:.4f}%)\")\n",
    "    print(f\"Loading and evaluating best model (l={best_l}) on the Test Set...\")\n",
    "    best_model = resnet18(weights=None)\n",
    "    best_model.fc = nn.Linear(best_model.fc.in_features, num_classes)\n",
    "    best_model.load_state_dict(torch.load(f\"{model_prefix}_l_{best_l}.pth\", weights_only=True))\n",
    "    best_model = best_model.to(device)\n",
    "    test_acc, _ = evaluate_model(best_model, test_loader, nn.CrossEntropyLoss())\n",
    "    print(f\"Final Test Accuracy (with best l={best_l} config): {test_acc:.4f}%\")\n",
    "    return test_acc, best_l\n",
    "\n",
    "# --- Main Experiment Loop ---\n",
    "def main():\n",
    "    # Load datasets\n",
    "    base_train_val_dataset = OxfordIIITPet(root='./dataset', split='trainval', target_types='category', download=True)\n",
    "    test_dataset_raw = OxfordIIITPet(root='./dataset', split='test', target_types='category', download=True)\n",
    "    # Split train/val\n",
    "    num_train_val_samples = len(base_train_val_dataset)\n",
    "    num_val_samples = int(VAL_FRACTION * num_train_val_samples)\n",
    "    num_train_samples_for_split = num_train_val_samples - num_val_samples\n",
    "    train_subset_raw, val_subset_raw = random_split(base_train_val_dataset, [num_train_samples_for_split, num_val_samples])\n",
    "    # Apply transforms\n",
    "    train_subset = TransformedDataset(train_subset_raw, transform=train_transform)\n",
    "    val_subset = TransformedDataset(val_subset_raw, transform=test_transform)\n",
    "    test_dataset = TransformedDataset(test_dataset_raw, transform=test_transform)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    results = []\n",
    "    for frac in FRACTIONS:\n",
    "        print(f\"\\n========== Fraction of labelled data: {frac*100:.1f}% ==========\")\n",
    "        # 1. Subsample labelled data\n",
    "        labelled_subset = subsample_dataset(train_subset, frac)\n",
    "        labelled_loader = DataLoader(labelled_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        # 2. Unlabelled data (the rest)\n",
    "        all_indices = set(range(len(train_subset)))\n",
    "        labelled_indices = set(labelled_subset.indices)\n",
    "        unlabelled_indices = list(all_indices - labelled_indices)\n",
    "        unlabelled_subset = Subset(train_subset, unlabelled_indices)\n",
    "        unlabelled_loader = DataLoader(unlabelled_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        # 3. Train using Strategy 1 on labelled data\n",
    "        t0 = time.time()\n",
    "        test_acc, best_l = run_strategy1(\n",
    "            labelled_loader, val_loader, test_loader, NUM_CLASSES, NUM_EPOCHS,\n",
    "            LR_FC, LR_BACKBONE, FACTOR, PATIENCE, L2_LAMBDA, f\"{MODEL_PREFIX}_frac{frac}\"\n",
    "        )\n",
    "        t1 = time.time()\n",
    "        print(f\"Strategy 1 training time: {(t1-t0)/60:.2f} minutes\")\n",
    "        # 4. If not 100%, do pseudo-labeling and retrain\n",
    "        if frac < 1.0 and len(unlabelled_subset) > 0:\n",
    "            print(\"\\n--- Pseudo-labeling phase ---\")\n",
    "            # Load best model from labelled training\n",
    "            model = resnet18(weights=None)\n",
    "            model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "            model.load_state_dict(torch.load(f\"{MODEL_PREFIX}_frac{frac}_l_{best_l}.pth\", weights_only=True))\n",
    "            model = model.to(device)\n",
    "            pseudo_labelled_dataset = generate_pseudo_labels(model, unlabelled_loader)\n",
    "            if pseudo_labelled_dataset is not None and len(pseudo_labelled_dataset) > 0:\n",
    "                combined_dataset = ConcatDataset([labelled_subset, pseudo_labelled_dataset])\n",
    "                combined_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "                test_acc_semi, _ = run_strategy1(\n",
    "                    combined_loader, val_loader, test_loader, NUM_CLASSES, NUM_EPOCHS,\n",
    "                    LR_FC, LR_BACKBONE, FACTOR, PATIENCE, L2_LAMBDA, f\"{MODEL_PREFIX}_frac{frac}_pseudo\"\n",
    "                )\n",
    "                test_acc = test_acc_semi  # Use the semi-supervised accuracy for plotting\n",
    "        results.append((frac, test_acc))\n",
    "    # --- Plot results ---\n",
    "    fractions, accuracies = zip(*results)\n",
    "    plt.plot([f*100 for f in fractions], accuracies, marker='o')\n",
    "    plt.xlabel('Percentage of Labelled Data Used')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title('Test Accuracy vs Labelled Data Fraction (with Pseudo-Labeling, Strategy 1)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b614d088-23e6-4480-952f-43f2c5d8f901",
   "metadata": {
    "tags": []
   },
   "source": [
    "| Labelled Fraction | Test Accuracy (Supervised) | Test Accuracy (Semi-supervised) |\n",
    "|---|---|---|\n",
    "| 1% | 42.90% | 51.51% |\n",
    "| 10% | 77.81% | 81.44% |\n",
    "| 50% | 89.02% | 88.96% |\n",
    "| 100% | 90.16% | ‚Äî |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f1f69-1036-4c6a-b7c6-0f9d52cf03dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# VITs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd628c-93cc-403e-a1d2-0a0444cbb6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets timm torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baff9a4c-3931-40da-95b1-c75731c503b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Loss: 3.5546 | Val Acc: 33.29%\n",
      "Epoch 2/30 | Loss: 3.1861 | Val Acc: 86.82%\n",
      "Epoch 3/30 | Loss: 2.3230 | Val Acc: 91.44%\n",
      "Epoch 4/30 | Loss: 1.3402 | Val Acc: 93.75%\n",
      "Epoch 5/30 | Loss: 0.7221 | Val Acc: 95.11%\n",
      "Epoch 6/30 | Loss: 0.4128 | Val Acc: 94.16%\n",
      "Epoch 7/30 | Loss: 0.2513 | Val Acc: 94.57%\n",
      "Epoch 8/30 | Loss: 0.1642 | Val Acc: 92.93%\n",
      "Epoch 9/30 | Loss: 0.1125 | Val Acc: 94.29%\n",
      "Epoch 10/30 | Loss: 0.0873 | Val Acc: 94.29%\n",
      "Epoch 11/30 | Loss: 0.0720 | Val Acc: 94.02%\n",
      "Epoch 12/30 | Loss: 0.0628 | Val Acc: 94.29%\n",
      "Epoch 13/30 | Loss: 0.0560 | Val Acc: 94.16%\n",
      "Epoch 14/30 | Loss: 0.0427 | Val Acc: 94.70%\n",
      "Epoch 15/30 | Loss: 0.0370 | Val Acc: 94.43%\n",
      "Epoch 16/30 | Loss: 0.0392 | Val Acc: 94.02%\n",
      "Epoch 17/30 | Loss: 0.0316 | Val Acc: 94.29%\n",
      "Epoch 18/30 | Loss: 0.0299 | Val Acc: 94.84%\n",
      "Epoch 19/30 | Loss: 0.0269 | Val Acc: 94.70%\n",
      "Epoch 20/30 | Loss: 0.0239 | Val Acc: 94.70%\n",
      "Epoch 21/30 | Loss: 0.0244 | Val Acc: 95.11%\n",
      "Epoch 22/30 | Loss: 0.0221 | Val Acc: 94.84%\n",
      "Epoch 23/30 | Loss: 0.0213 | Val Acc: 94.84%\n",
      "Epoch 24/30 | Loss: 0.0207 | Val Acc: 94.70%\n",
      "Epoch 25/30 | Loss: 0.0202 | Val Acc: 94.70%\n",
      "Epoch 26/30 | Loss: 0.0200 | Val Acc: 94.70%\n",
      "Epoch 27/30 | Loss: 0.0199 | Val Acc: 94.57%\n",
      "Epoch 28/30 | Loss: 0.0197 | Val Acc: 94.70%\n",
      "Epoch 29/30 | Loss: 0.0197 | Val Acc: 94.70%\n",
      "Epoch 30/30 | Loss: 0.0196 | Val Acc: 94.70%\n",
      "Best validation accuracy: 95.1086956521739\n",
      "Training time: 49.73 minutes\n",
      "Final Test Accuracy: 92.34%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --- 1. Load ViT processor and define transforms ---\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "mean = processor.image_mean\n",
    "std = processor.image_std\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# --- 2. Load Oxford-IIIT Pet dataset (multi-class) ---\n",
    "full_trainval_train = OxfordIIITPet(root='./dataset', split='trainval', target_types='category', transform=train_transform, download=True)\n",
    "full_trainval_val = OxfordIIITPet(root='./dataset', split='trainval', target_types='category', transform=test_transform, download=True)\n",
    "test_set = OxfordIIITPet(root='./dataset', split='test', target_types='category', transform=test_transform, download=True)\n",
    "\n",
    "# --- 3. Split train/val indices ---\n",
    "val_fraction = 0.2\n",
    "num_val = int(len(full_trainval_train) * val_fraction)\n",
    "num_train = len(full_trainval_train) - num_val\n",
    "indices = np.random.permutation(len(full_trainval_train))\n",
    "train_indices, val_indices = indices[:num_train], indices[num_train:]\n",
    "\n",
    "train_set = Subset(full_trainval_train, train_indices)\n",
    "val_set = Subset(full_trainval_val, val_indices)\n",
    "\n",
    "# --- 4. DataLoaders ---\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "num_classes = 37\n",
    "\n",
    "# --- 5. Load ViT model ---\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# --- 6. Optimizer, Scheduler, Loss (LLRD) ---\n",
    "epochs = 30\n",
    "lr_backbone = 5e-5\n",
    "lr_classifier = 2e-4\n",
    "optimizer = AdamW([\n",
    "    {\"params\": model.vit.parameters(), \"lr\": lr_backbone},\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": lr_classifier}\n",
    "], weight_decay=1e-4)\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# --- 7. Training & Evaluation ---\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs).logits\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        val_acc = evaluate(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(train_loader):.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"vit_best_model.pth\")\n",
    "    print(\"Best validation accuracy:\", best_val_acc)\n",
    "\n",
    "start = time.time()\n",
    "train(model, train_loader, val_loader, epochs)\n",
    "print(\"Training time: %.2f minutes\" % ((time.time()-start)/60))\n",
    "\n",
    "# --- 8. Test Evaluation ---\n",
    "model.load_state_dict(torch.load(\"vit_best_model.pth\"))\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Final Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2883407-f42b-4b53-bb0b-f15ddd01a847",
   "metadata": {},
   "source": [
    "**Test Accuracy**: 92.34%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4da14-442a-4bb2-a1fe-98146fba6b15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LoRA layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078f2a3-739e-4bb0-8131-bfdb91f465fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import math\n",
    "import time\n",
    "\n",
    "# 1. Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. LoRA module for Conv2d layers (used only in layer4)\n",
    "class LoRAConv2d(nn.Module):\n",
    "    def __init__(self, conv, r=8, alpha=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = conv\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        # Use 1x1 convs for LoRA adapters (to match Hugo's implementation)\n",
    "        in_channels = conv.in_channels\n",
    "        out_channels = conv.out_channels\n",
    "        \n",
    "        kernel_size = conv.kernel_size\n",
    "        padding = conv.padding\n",
    "        \n",
    "        stride = conv.stride\n",
    "        dilation = conv.dilation\n",
    "        self.lora_A = nn.Conv2d(in_channels, r, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=False)\n",
    "\n",
    "        #self.lora_A = nn.Conv2d(in_channels, r, kernel_size=1, bias=False)\n",
    "        self.lora_B = nn.Conv2d(r, out_channels, kernel_size=1, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        # Freeze the original conv weights and bias\n",
    "        for param in self.conv.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.conv(x)\n",
    "        lora_out = self.dropout(x)\n",
    "        lora_out = self.lora_A(lora_out)\n",
    "        lora_out = self.lora_B(lora_out)\n",
    "        result = result + self.scaling * lora_out\n",
    "        return result\n",
    "\n",
    "\n",
    "# Count all parameters in the model (before freezing)\n",
    "def count_all_params(model, msg=\"Total parameters in model: \"):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{msg} {total:,} (trainable: {trainable:,})\")\n",
    "\n",
    "# Count trainable parameters in layer4\n",
    "def count_layer4_params(model, msg=\"Trainable parameters in layer4: \"):\n",
    "    layer4_params = sum(p.numel() for p in model.layer4.parameters() if p.requires_grad)\n",
    "    print(f\"{msg} {layer4_params:,}\")\n",
    "\n",
    "\n",
    "def count_lora_params(model):\n",
    "    lora_params = 0\n",
    "    for layer in [model.layer2, model.layer3, model.layer4]:\n",
    "        for block in layer:\n",
    "            if isinstance(block.conv2, LoRAConv2d):\n",
    "                lora_params += sum(p.numel() for p in block.conv2.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable LoRA parameters in layer2, layer3, layer4: {lora_params:,}\")\n",
    "\n",
    "# 3. Data transforms (augmentation for train, normalization for all)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.75, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 4. Load dataset and split train/val\n",
    "full_train_dataset = OxfordIIITPet(root='./dataset', split='trainval', target_types='category', download=True)\n",
    "test_dataset = OxfordIIITPet(root='./dataset', split='test', target_types='category', transform=test_transform, download=True)\n",
    "\n",
    "val_fraction = 0.2\n",
    "num_val = int(len(full_train_dataset) * val_fraction)\n",
    "num_train = len(full_train_dataset) - num_val\n",
    "train_subset, val_subset = random_split(full_train_dataset, [num_train, num_val])\n",
    "\n",
    "# 5. Apply transforms to subsets\n",
    "class TransformedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        return self.transform(x), y\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "train_dataset = TransformedDataset(train_subset, train_transform)\n",
    "val_dataset = TransformedDataset(val_subset, test_transform)\n",
    "\n",
    "# 6. Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "\"\"\"# 7. Model setup: LoRA only in layer4, unfreeze last 2 blocks\n",
    "model = resnet50(weights='IMAGENET1K_V1')\n",
    "count_all_params(model, msg=\"All parameters in model before freezing: \")\n",
    "\n",
    "# Freeze all parameters first\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Count before LoRA\n",
    "#count_layer4_params(model, msg=\"Trainable in layer4 before LoRA: \")\n",
    "\n",
    "# Replace Conv2d layers in layer4 with LoRA-augmented Conv2d\n",
    "# for layer in [model.layer2, model.layer3, model.layer4]:\n",
    "#     for block in layer:\n",
    "#         block.conv2 = LoRAConv2d(block.conv2, r=8, alpha=16, dropout=0.1)\n",
    "\n",
    "# Count after LoRA\n",
    "#count_layer4_params(model, msg=\"Trainable in layer4 after LoRA: \")\n",
    "# After wrapping conv2 in all layers with LoRAConv2d\n",
    "#count_lora_params(model)\n",
    "\n",
    "# Use standard fully connected layer (no LoRA in fc)\n",
    "model.fc = nn.Linear(model.fc.in_features, 37)\n",
    "\n",
    "# 8. Optimizer and loss setup (layer-wise learning rates)\n",
    "params = [\n",
    "    {'params': model.fc.parameters(), 'lr': 1e-3},\n",
    "    {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.layer3.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.layer2.parameters(), 'lr': 1e-4},\n",
    "]\n",
    "optimizer = optim.Adam(params, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1)\n",
    "\n",
    "model = model.to(device)\"\"\"\n",
    "\n",
    "\n",
    "# 9. Training with validation and checkpointing\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=15):\n",
    "    best_val_acc = 0\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        val_acc = test(model, val_loader)\n",
    "        scheduler.step(val_acc)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        # Print GPU memory usage after each epoch (if using CUDA)\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "            print(f\"GPU max memory allocated: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return best_val_acc\n",
    "\n",
    "# 10. Evaluation function\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "# 11. Run training and evaluation\n",
    "# start_time = time.time()\n",
    "# print(\"Starting improved ResNet18+LoRA (layer4 only) fine-tuning...\")\n",
    "# best_val_acc = train(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=10)\n",
    "# test_acc = test(model, test_loader)\n",
    "# print(f\"Best Val Accuracy: {best_val_acc:.2f}%\")\n",
    "# print(f\"Test Accuracy (best model): {test_acc:.2f}%\")\n",
    "\n",
    "# end_time = time.time()\n",
    "# elapsed = end_time - start_time\n",
    "# print(f\"Total computation time: {elapsed/60:.2f} minutes ({elapsed:.2f} seconds)\")\n",
    "\n",
    "experiment_settings = [\n",
    "    #(4, 4),\n",
    "    (4, 8),\n",
    "    #(8, 8),\n",
    "    #(8, 16),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for r, alpha in experiment_settings:\n",
    "    print(f\"\\n=== Running with r={r}, alpha={alpha} ===\")\n",
    "    # Re-initialize model and freeze all params\n",
    "    model = resnet34(weights='IMAGENET1K_V1')\n",
    "    \"\"\"print(\"Conv2 parameters:\")\n",
    "    dor layer in [model.layer2, model.layer3, model.layer4]:\n",
    "        for block in layer:\n",
    "            print(f\"  {layer.__class__.__name__}.{block.__class__.__name__}:\")\n",
    "            print(f\"    in_channels: {block.conv2.in_channels}\")\n",
    "            print(f\"    out_channels: {block.conv2.out_channels}\")\n",
    "            print(f\"    kernel_size: {block.conv2.kernel_size}\")\n",
    "            print(f\"    stride: {block.conv2.stride}\")\n",
    "            print(f\"    padding: {block.conv2.padding}\")\n",
    "            print(f\"    dilation: {block.conv2.dilation}\")\"\"\"\n",
    "            \n",
    "    #count_all_params(model, msg=\"All parameters in model before freezing: \")\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Apply LoRA to conv2 in layer2, layer3, layer4\n",
    "    for layer in [model.layer2, model.layer3, model.layer4]:\n",
    "        for block in layer:\n",
    "            block.conv2 = LoRAConv2d(block.conv2, r=r, alpha=alpha, dropout=0.1)\n",
    "\n",
    "    count_lora_params(model)\n",
    "    # Replace FC layer\n",
    "    model.fc = nn.Linear(model.fc.in_features, 37)\n",
    "    fc_params = sum(p.numel() for p in model.fc.parameters())\n",
    "    print(f\"Number of parameters in the fc layer: {fc_params:,}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    params = [\n",
    "        {'params': model.fc.parameters(), 'lr': 1e-3},\n",
    "        {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.layer3.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.layer2.parameters(), 'lr': 1e-4},\n",
    "    ]\n",
    "    optimizer = optim.Adam(params, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train and evaluate\n",
    "    best_val_acc = train(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=15)\n",
    "    test_acc = test(model, test_loader)\n",
    "    print(f\"r={r}, alpha={alpha} -> Best Val Acc: {best_val_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
    "    results.append((r, alpha, best_val_acc, test_acc))\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Total computation time: {elapsed/60:.2f} minutes ({elapsed:.2f} seconds)\")\n",
    "\n",
    "print(\"\\nSummary of results:\")\n",
    "for r, alpha, val_acc, test_acc in results:\n",
    "    print(f\"r={r}, alpha={alpha} -> Val: {val_acc:.2f}%, Test: {test_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
